name: SEACE CSV ZIP -> BigQuery (3 meses FULL CLEAN + carga + union estable) [DEBUG]

on:
  workflow_dispatch: {}

jobs:
  seace_3months_full:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      MONTHS: "2025-11 2025-12 2026-01"
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth Google
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # 0) Crear dataset si no existe
      - name: Crear dataset si no existe
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" || true

      # 1) LIMPIEZA TOTAL BigQuery (tablas + vistas)
      - name: LIMPIEZA TOTAL BigQuery (tablas + vistas)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, subprocess

          PROJECT = os.environ["PROJECT_ID"]
          DATASET = os.environ["DATASET"]

          # Usamos INFORMATION_SCHEMA para listar tablas y vistas sin depender del formato de bq ls.
          q = f"""
          SELECT table_name, table_type
          FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.TABLES`
          """
          out = subprocess.check_output(
              ["bq","--project_id",PROJECT,"query","--use_legacy_sql=false","--format=csv",q],
              text=True
          )
          rows = [ln.split(",") for ln in out.splitlines()[1:] if ln.strip()]
          names = [r[0].strip() for r in rows if r and r[0].strip()]

          if not names:
              print("Dataset ya estaba vacío.")
          else:
              for name in names:
                  print(f"Borrando: {DATASET}.{name}")
                  subprocess.check_call(["bq","--project_id",PROJECT,"rm","-f","-t",f"{DATASET}.{name}"])

          print("OK: limpieza BigQuery completa.")
          PY

      # 2) Limpieza runner
      - name: Limpiar runner (data/ tmp/)
        shell: bash
        run: |
          set -euo pipefail
          rm -rf data tmp || true
          mkdir -p data tmp
          echo "Runner limpio."

      # 3) Descargar ZIPs + extraer + contar filas reales + expected.tsv
      - name: Descargar ZIPs + extraer + conteo REAL + expected.tsv
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, zipfile, subprocess
          from pathlib import Path
          from collections import defaultdict

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp"); tmp.mkdir(exist_ok=True)

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              if r.returncode != 0:
                  print(r.stdout)
                  raise SystemExit(r.returncode)

          def count_lines_fast(path: Path) -> int:
              with path.open("rb") as f:
                  return sum(1 for _ in f)

          expected_lines = ["family\tym\texpected_rows"]

          for ym in MONTHS:
              y, m = ym.split("-")
              url = f"https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/{y}/{m}"
              zip_path = tmp / f"seace_{y}_{m}.zip"
              out_dir  = tmp / f"unzipped_{y}_{m}"

              print(f"\n=== MES {ym} ===")
              print(f"URL: {url}")

              run(["bash","-lc", f"curl -L -f -sS '{url}' -o '{zip_path}'"])
              if zip_path.stat().st_size == 0:
                  raise SystemExit(f"ZIP vacío: {zip_path}")

              out_dir.mkdir(parents=True, exist_ok=True)
              with zipfile.ZipFile(zip_path, "r") as z:
                  z.extractall(out_dir)

              csvs = sorted(out_dir.glob("*.csv"))
              if not csvs:
                  raise SystemExit(f"No hay CSVs en {zip_path}")

              per_family = defaultdict(int)
              for f in csvs:
                  total_lines = count_lines_fast(f)
                  data_rows = max(total_lines - 1, 0)
                  fam = f.stem
                  per_family[fam] += data_rows
                  print(f"{f.name:35s} total_lines={total_lines:8d} data_rows={data_rows:8d}")

              print("\n--- Total por familia (este mes) ---")
              for fam, rows in sorted(per_family.items()):
                  print(f"{fam:35s} data_rows_sum={rows:10d}")
                  expected_lines.append(f"{fam}\t{ym}\t{rows}")

          (tmp/"expected.tsv").write_text("\n".join(expected_lines) + "\n", encoding="utf-8")
          print("\nOK: tmp/expected.tsv creado")
          PY

      - name: Mostrar expected.tsv (primeras 120 líneas)
        shell: bash
        run: |
          set -euo pipefail
          sed -n '1,120p' tmp/expected.tsv

      # 4) Normalizar headers + schema TODO STRING + guardar CSV limpio en data/
      - name: Normalizar headers + schema TODO STRING
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, json
          from pathlib import Path

          MONTHS = os.environ["MONTHS"].split()
          tmp  = Path("tmp")
          data = Path("data"); data.mkdir(exist_ok=True)

          def norm_name(s: str) -> str:
              s = s.strip().replace("/", "_").replace(" ", "_").replace("-", "_")
              s = re.sub(r"[^A-Za-z0-9_]", "_", s)
              s = re.sub(r"_+", "_", s).strip("_")
              if not s:
                  s = "col"
              if s[0].isdigit():
                  s = "c_" + s
              return s

          def normalize_header(line: str):
              cols = line.rstrip("\n\r").split(",")
              fixed = []
              seen = {}
              for c in cols:
                  c = norm_name(c)
                  k = seen.get(c, 0)
                  seen[c] = k + 1
                  if k > 0:
                      c = f"{c}_{k}"
                  fixed.append(c)
              return fixed

          total = 0
          for ym in MONTHS:
              y, m = ym.split("-")
              suffix = f"{y}_{m}"
              src_dir = tmp / f"unzipped_{y}_{m}"

              for src in sorted(src_dir.glob("*.csv")):
                  base = norm_name(src.stem)
                  table = f"{base}_{suffix}"

                  b = src.read_bytes().replace(b"\x00", b"")
                  lines = b.decode("utf-8", errors="replace").splitlines(True)
                  if not lines:
                      continue

                  header_cols = normalize_header(lines[0])
                  lines[0] = ",".join(header_cols) + "\n"

                  out_csv = data / f"{table}.csv"
                  out_csv.write_text("".join(lines), encoding="utf-8", newline="")

                  schema = [{"name": c, "type": "STRING", "mode": "NULLABLE"} for c in header_cols]
                  (data / f"{table}.schema.json").write_text(json.dumps(schema), encoding="utf-8")

                  total += 1

          print(f"OK: CSVs limpios creados: {total}")
          PY

      - name: Ver data/ (muestra segura)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          from pathlib import Path
          p = Path("data")
          files = sorted([f.name for f in p.iterdir()])
          print(f"Archivos en data/: {len(files)}")
          for f in files[:80]:
              print(f)
          PY

      # 5) Cargar CSVs a BigQuery CON CAPTURA TOTAL DE ERRORES
      - name: Cargar CSVs a BigQuery (TODO STRING) - con logging de errores
        shell: bash
        run: |
          set -uo pipefail
          mkdir -p tmp
          : > tmp/load_ok.log
          : > tmp/load_errors.log

          failed=0

          for csv in data/*.csv; do
            table=$(basename "$csv" .csv)

            echo "[LOAD] $table"
            out=$(bq --project_id="$PROJECT_ID" load \
              --replace \
              --source_format=CSV \
              --skip_leading_rows=1 \
              --allow_quoted_newlines \
              --max_bad_records=0 \
              "$DATASET.$table" \
              "$csv" \
              "data/$table.schema.json" 2>&1)

            rc=$?
            if [ $rc -ne 0 ]; then
              failed=1
              echo "[FAIL] $table (rc=$rc)" | tee -a tmp/load_errors.log
              echo "$out" | tee -a tmp/load_errors.log
              echo "----------------------------------------" >> tmp/load_errors.log
            else
              echo "[OK] $table" | tee -a tmp/load_ok.log
            fi
          done

          echo "=== RESUMEN CARGA ==="
          echo "OK:   $(wc -l < tmp/load_ok.log || echo 0)"
          echo "FAIL: $(grep -c '^\[FAIL\]' tmp/load_errors.log || echo 0)"

          # FALLA EL JOB SI HUBO ERRORES DE CARGA (recomendado para que no te “llegue al final” mintiendo)
          if [ $failed -ne 0 ]; then
            echo "Hubo errores de carga. Revisa tmp/load_errors.log"
            exit 1
          fi

      - name: Mostrar errores de carga (si existen)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -s tmp/load_errors.log ]; then
            echo "=== ERRORES DE CARGA (primeras 250 líneas) ==="
            sed -n '1,250p' tmp/load_errors.log
          else
            echo "Sin errores de carga."
          fi

      # 5.1) Inventario de tablas cargadas (para ver qué llegó realmente)
      - name: Inventario de tablas cargadas (después de load)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false --format=pretty "
          SELECT table_name, row_count
          FROM \`${PROJECT_ID}.${DATASET}.__TABLES__\`
          ORDER BY table_name
          "

      # 6) Unión por familia con logging por familia (y listando mensuales por INFORMATION_SCHEMA)
      - name: Unir tablas por familia (estable) - con logging por familia
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, subprocess
          from collections import defaultdict
          from pathlib import Path

          PROJECT = os.environ["PROJECT_ID"]
          DATASET = os.environ["DATASET"]

          tmp = Path("tmp")
          tmp.mkdir(exist_ok=True)
          (tmp / "union_ok.log").write_text("", encoding="utf-8")
          (tmp / "union_errors.log").write_text("", encoding="utf-8")

          def bq_query_csv(sql: str) -> str:
              return subprocess.check_output(
                  ["bq","--project_id",PROJECT,"query","--use_legacy_sql=false","--format=csv",sql],
                  text=True
              )

          # LISTAR TABLAS MENSUALES EXISTENTES
          q_monthly = f"""
          SELECT table_name
          FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.TABLES`
          WHERE table_type='BASE TABLE'
            AND REGEXP_CONTAINS(table_name, r'_[0-9]{{4}}_[0-9]{{2}}$')
          """
          out = bq_query_csv(q_monthly)
          monthly = [ln.strip() for ln in out.splitlines()[1:] if ln.strip()]

          if not monthly:
              raise SystemExit("No hay tablas mensuales *_YYYY_MM para unir.")

          fams = defaultdict(list)
          for t in monthly:
              fam = re.sub(r'_[0-9]{4}_[0-9]{2}$', '', t)
              fams[fam].append(t)

          def get_cols(table: str):
              q = f"""
              SELECT column_name
              FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.COLUMNS`
              WHERE table_name = '{table}'
              ORDER BY ordinal_position
              """
              out = bq_query_csv(q)
              return [ln.strip() for ln in out.splitlines()[1:] if ln.strip()]

          failed = []

          for fam, ts in sorted(fams.items()):
              ts = sorted(ts)
              try:
                  all_cols = []
                  seen = set()
                  cols_by_table = {}

                  for t in ts:
                      cols = get_cols(t)
                      cols_by_table[t] = set(cols)
                      for c in cols:
                          if c not in seen:
                              seen.add(c)
                              all_cols.append(c)

                  selects = []
                  for t in ts:
                      present = cols_by_table[t]
                      exprs = []
                      for c in all_cols:
                          if c in present:
                              exprs.append(f"`{c}` AS `{c}`")
                          else:
                              exprs.append(f"CAST(NULL AS STRING) AS `{c}`")
                      selects.append(f"SELECT {', '.join(exprs)} FROM `{PROJECT}.{DATASET}.{t}`")

                  union_sql = " UNION ALL ".join(selects)
                  final_sql = f"CREATE OR REPLACE TABLE `{PROJECT}.{DATASET}.{fam}` AS {union_sql}"

                  print(f"[UNION] {fam} <= {len(ts)} tablas | cols={len(all_cols)}")
                  subprocess.check_call(["bq","--project_id",PROJECT,"query","--use_legacy_sql=false",final_sql])

                  with (tmp / "union_ok.log").open("a", encoding="utf-8") as f:
                      f.write(f"OK\t{fam}\t{len(ts)}\t{len(all_cols)}\t" + ",".join(ts) + "\n")

              except subprocess.CalledProcessError as e:
                  failed.append(fam)
                  with (tmp / "union_errors.log").open("a", encoding="utf-8") as f:
                      f.write(f"FAIL\t{fam}\t{len(ts)}\t{e}\t" + ",".join(ts) + "\n")
                  print(f"[ERROR UNION] {fam}: {e}")

          print("\n=== RESUMEN UNION ===")
          ok_lines = (tmp / "union_ok.log").read_text(encoding="utf-8").strip().splitlines()
          err_lines = (tmp / "union_errors.log").read_text(encoding="utf-8").strip().splitlines()
          print(f"OK:   {0 if ok_lines==[''] else len(ok_lines)}")
          print(f"FAIL: {0 if err_lines==[''] else len(err_lines)}")

          if failed:
              raise SystemExit("Fallaron uniones: " + ", ".join(failed))
          PY

      - name: Mostrar errores de unión (si existen)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -s tmp/union_errors.log ]; then
            echo "=== ERRORES DE UNION ==="
            cat tmp/union_errors.log
          else
            echo "Sin errores de unión."
          fi

      # 7) Diagnóstico completo: qué familias y qué meses realmente existen
      - name: Diagnóstico por familia (qué meses existen por tabla)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false --format=pretty "
          WITH t AS (
            SELECT table_name
            FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
            WHERE table_type='BASE TABLE'
          ),
          monthly AS (
            SELECT
              REGEXP_REPLACE(table_name, r'_[0-9]{4}_[0-9]{2}$', '') AS fam,
              REGEXP_EXTRACT(table_name, r'_([0-9]{4}_[0-9]{2})$') AS ym
            FROM t
            WHERE REGEXP_CONTAINS(table_name, r'_[0-9]{4}_[0-9]{2}$')
          )
          SELECT fam, ARRAY_AGG(ym ORDER BY ym) AS meses
          FROM monthly
          GROUP BY fam
          ORDER BY fam;
          "

      # 7.1) Diagnóstico brutal: familias que esperabas (expected.tsv) vs lo que existe realmente
      - name: expected.tsv vs tablas existentes (detecta TODO lo que falta)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, subprocess
          from pathlib import Path

          PROJECT = os.environ["PROJECT_ID"]
          DATASET = os.environ["DATASET"]

          expected_path = Path("tmp/expected.tsv")
          if not expected_path.exists():
              print("No existe tmp/expected.tsv, no puedo comparar.")
              raise SystemExit(1)

          # Leer expected (family, ym)
          expected = set()
          lines = expected_path.read_text(encoding="utf-8").splitlines()
          for ln in lines[1:]:
              if not ln.strip():
                  continue
              fam, ym, _rows = ln.split("\t")
              y, m = ym.split("-")
              expected.add(f"{fam}_{y}_{m}")

          # Listar tablas existentes mensuales en BQ
          q = f"""
          SELECT table_name
          FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.TABLES`
          WHERE table_type='BASE TABLE'
            AND REGEXP_CONTAINS(table_name, r'_[0-9]{{4}}_[0-9]{{2}}$')
          """
          out = subprocess.check_output(["bq","--project_id",PROJECT,"query","--use_legacy_sql=false","--format=csv",q], text=True)
          existing = set([ln.strip() for ln in out.splitlines()[1:] if ln.strip()])

          missing = sorted(expected - existing)
          extra   = sorted(existing - expected)

          Path("tmp/missing_monthly_tables.log").write_text("\n".join(missing) + ("\n" if missing else ""), encoding="utf-8")
          Path("tmp/extra_monthly_tables.log").write_text("\n".join(extra) + ("\n" if extra else ""), encoding="utf-8")

          print("=== COMPARACION expected vs existing ===")
          print(f"Expected monthly tables: {len(expected)}")
          print(f"Existing monthly tables: {len(existing)}")
          print(f"Missing monthly tables:  {len(missing)}  -> tmp/missing_monthly_tables.log")
          print(f"Extra monthly tables:    {len(extra)}    -> tmp/extra_monthly_tables.log")

          # Si faltan, marca el job como fallido (para que no “llegue al final” con dataset incompleto)
          if missing:
              raise SystemExit("FALTAN TABLAS MENSUALES. Ver tmp/missing_monthly_tables.log")
          PY

      - name: Mostrar missing_monthly_tables.log (si existe)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -s tmp/missing_monthly_tables.log ]; then
            echo "=== TABLAS MENSUALES FALTANTES (primeras 200) ==="
            sed -n '1,200p' tmp/missing_monthly_tables.log
          else
            echo "No faltan tablas mensuales según expected.tsv"
          fi

      # 8) Verificación rápida (records si existe) - no revienta el job
      - name: Verificar records final (conteo, si existe)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false --format=pretty "
          SELECT COUNT(1) AS filas_records_final
          FROM \`${PROJECT_ID}.${DATASET}.records\`
          " || true
