name: SEACE CSV ZIP -> BigQuery (3 meses, sin mezcla, limpieza estricta)

on:
  workflow_dispatch: {}

# Evita que se mezclen ejecuciones del mismo workflow (se encolan)
concurrency:
  group: seace-bq-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  seace_zip_3months_to_bq:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # Paso 1: Limpieza local REAL (antes de empezar)
      - name: Limpiar data/ y tmp/
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p data tmp
          rm -rf data/* tmp/*
          mkdir -p tmp/zips
          echo "OK: data/ y tmp/ limpios"

      # Paso 2: Descargar ZIPs (sip) (3 meses)
      - name: Descargar ZIPs SEACE (csv) 3 meses
        shell: bash
        run: |
          set -euxo pipefail

          # Cambia aquí tus 3 meses (YYYY-MM)
          MONTHS=("2026-01" "2025-12" "2025-11")

          for ym in "${MONTHS[@]}"; do
            YEAR="${ym%-*}"
            MONTH="${ym#*-}"

            URL="https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/${YEAR}/${MONTH}"
            ZIP="tmp/zips/seace_${YEAR}_${MONTH}.zip"

            echo "Descargando: $URL"
            curl -L -f -sS "$URL" -o "$ZIP"
            test -s "$ZIP"

            echo "OK ZIP: $ZIP"
          done

      # Paso 3: Crear dataset + (opcional) borrarlo y recrearlo limpio
      - name: Preparar dataset BigQuery (bígcueri) limpio
        shell: bash
        run: |
          set -euxo pipefail
          PROJECT_ID="heroic-ruler-481618-e5"
          DATASET="github_actions"

          # Si quieres limpieza TOTAL de tablas anteriores, deja esto activado:
          bq --project_id="$PROJECT_ID" rm -r -f -d "$PROJECT_ID:$DATASET" || true
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET"

          echo "OK: Dataset listo y limpio"

      # Paso 4: Procesar mes por mes SIN MEZCLA (limpia entre meses)
      - name: Procesar meses en orden (sin mezcla) y cargar a BigQuery
        shell: bash
        run: |
          set -euxo pipefail

          python - <<'PY'
          import os, re, glob, shutil, subprocess, sys

          PROJECT_ID = "heroic-ruler-481618-e5"
          DATASET     = "github_actions"

          # Orden explícito (para que no “decida” el filesystem)
          MONTHS = ["2026-01", "2025-12", "2025-11"]

          ZIPS_DIR = "tmp/zips"
          UNZIP_DIR = "tmp/unzipped"
          DATA_DIR = "data"

          os.makedirs(ZIPS_DIR, exist_ok=True)
          os.makedirs(DATA_DIR, exist_ok=True)

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              print(r.stdout)
              return r.returncode

          def normalize_headers_line(header: str):
              cols = header.rstrip("\n\r").split(",")
              fixed = []
              seen = {}
              for c in cols:
                  c = c.strip()
                  c = c.replace("/", "_").replace(" ", "_").replace("-", "_")
                  c = re.sub(r"[^A-Za-z0-9_]", "_", c)
                  c = re.sub(r"_+", "_", c).strip("_")

                  if not c:
                      c = "col"
                  if c[0].isdigit():
                      c = "c_" + c

                  base = c
                  k = seen.get(base, 0)
                  if k > 0:
                      c = f"{base}_{k}"
                  seen[base] = k + 1
                  fixed.append(c)

              return ",".join(fixed) + "\n"

          def safe_table_name(name_no_ext: str) -> str:
              t = re.sub(r"[^A-Za-z0-9_]", "_", name_no_ext)
              t = re.sub(r"_+", "_", t).strip("_")
              if not t:
                  t = "tabla"
              if t[0].isdigit():
                  t = "t_" + t
              return t

          # Controla replace la primera vez y append después (se mantiene entre meses)
          created_tables = set()

          for ym in MONTHS:
              year, month = ym.split("-")
              zip_path = os.path.join(ZIPS_DIR, f"seace_{year}_{month}.zip")
              if not os.path.exists(zip_path) or os.path.getsize(zip_path) == 0:
                  print(f"[ERROR] ZIP faltante o vacío: {zip_path}")
                  sys.exit(1)

              print(f"\n==============================")
              print(f"== PROCESANDO MES: {ym}")
              print(f"==============================")

              # LIMPIEZA ENTRE MESES: borrar unzip y data antes de generar/cargar
              if os.path.exists(UNZIP_DIR):
                  shutil.rmtree(UNZIP_DIR)
              os.makedirs(UNZIP_DIR, exist_ok=True)

              # Limpia data/ para no mezclar archivos de salida entre meses
              for f in glob.glob(os.path.join(DATA_DIR, "*")):
                  os.remove(f)

              # 1) Unzip
              rc = run(["unzip", "-o", zip_path, "-d", UNZIP_DIR])
              if rc != 0:
                  print(f"[ERROR] unzip falló para {zip_path}")
                  sys.exit(rc)

              # 2) Encontrar CSVs de ESTE mes
              csv_paths = sorted(glob.glob(os.path.join(UNZIP_DIR, "**", "*.csv"), recursive=True))
              if not csv_paths:
                  print(f"[WARN] No hay CSVs en {zip_path}. Se salta este mes.")
                  continue

              # 3) Procesar cada CSV y cargarlo
              for src in csv_paths:
                  base = os.path.basename(src)
                  table = safe_table_name(os.path.splitext(base)[0])
                  out_clean = os.path.join(DATA_DIR, f"{table}__{ym}.csv")

                  with open(src, "rb") as f:
                      b = f.read().replace(b"\x00", b"")

                  lines = b.decode("utf-8", errors="replace").splitlines(True)
                  if not lines:
                      print(f"[SKIP] vacío: {src}")
                      continue

                  # Header normalizado + columna anio_mes
                  norm_header = normalize_headers_line(lines[0])
                  new_lines = ["anio_mes," + norm_header]

                  for line in lines[1:]:
                      if line.strip() == "":
                          continue
                      new_lines.append(f"{ym}," + line)

                  with open(out_clean, "w", encoding="utf-8", newline="") as g:
                      g.writelines(new_lines)

                  full_table = f"{DATASET}.{table}"

                  mode_flag = "--replace" if table not in created_tables else "--append"

                  cmd = [
                      "bq", f"--project_id={PROJECT_ID}", "load",
                      "--source_format=CSV",
                      "--autodetect",
                      "--skip_leading_rows=1",
                      "--allow_quoted_newlines",
                      "--allow_jagged_rows",
                      "--max_bad_records=5000",
                      "--schema_update_option=ALLOW_FIELD_ADDITION",
                      "--schema_update_option=ALLOW_FIELD_RELAXATION",
                      mode_flag,
                      full_table,
                      out_clean
                  ]

                  print(f"\n=== {mode_flag} {base} -> {full_table} (anio_mes={ym}) ===")
                  rc = run(cmd)
                  if rc != 0:
                      print(f"[ERROR] Carga fallida: {src} -> {full_table}")
                      sys.exit(rc)

                  created_tables.add(table)

              # LIMPIEZA POST-MES: borrar UNZIP y data para garantizar “no mezcla”
              shutil.rmtree(UNZIP_DIR, ignore_errors=True)
              for f in glob.glob(os.path.join(DATA_DIR, "*")):
                  os.remove(f)

              print(f"OK: Mes {ym} terminado y limpiado (sin mezcla).")

          print("\nOK: Proceso completo. Meses cargados y combinados por tabla.")
          PY
