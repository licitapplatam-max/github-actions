name: SEACE CSV ZIP -> BigQuery (tablas por mes + views + verificación)

on:
  workflow_dispatch: {}

jobs:
  seace_zip_to_bq:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      YEAR: "2026"
      MONTH: "01"

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # Paso 0: Crear dataset si no existe + borrar TODAS las tablas (sin borrar el dataset)
      - name: Limpiar BigQuery (borrar tablas, no dataset)
        shell: bash
        run: |
          set -euxo pipefail
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" || true

          TABLES=$(bq --project_id="$PROJECT_ID" ls "$DATASET" | tail -n +3 | awk '{print $1}' || true)
          if [ -z "${TABLES}" ]; then
            echo "No hay tablas para borrar en $PROJECT_ID:$DATASET"
          else
            echo "Borrando tablas en $PROJECT_ID:$DATASET:"
            echo "${TABLES}"
            echo "${TABLES}" | while read -r T; do
              if [ -n "$T" ]; then
                bq --project_id="$PROJECT_ID" rm -f "$DATASET.$T"
              fi
            done
          fi

      # Paso 1: Limpieza local del runner
      - name: Limpiar data/ y tmp/ (runner)
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p data tmp tmp/unzipped
          rm -rf data/* || true
          rm -rf tmp/*  || true
          mkdir -p tmp/unzipped
          echo "data/ limpio:"
          ls -lah data || true

      # Paso 2: Descargar ZIP CSV (1 mes)
      - name: Descargar ZIP SEACE (csv)
        shell: bash
        run: |
          set -euxo pipefail

          URL="https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/${YEAR}/${MONTH}"
          ZIP="tmp/seace_${YEAR}_${MONTH}.zip"

          echo "Descargando: $URL"
          curl -L -f -sS "$URL" -o "$ZIP"
          test -s "$ZIP"

          echo "Tipo de archivo:"
          file "$ZIP"

          echo "Contenido ZIP (primeros 120):"
          unzip -l "$ZIP" | head -n 120

      # Paso 3: Extraer CSVs
      - name: Extraer CSVs
        shell: bash
        run: |
          set -euxo pipefail
          ZIP="tmp/seace_${YEAR}_${MONTH}.zip"
          unzip -o "$ZIP" -d "tmp/unzipped"

          echo "CSVs encontrados:"
          find tmp/unzipped -type f -iname "*.csv" -print

          COUNT=$(find tmp/unzipped -type f -iname "*.csv" | wc -l)
          echo "Total CSV: $COUNT"
          test "$COUNT" -gt 0

          echo "Resumen tamaño + filas (top 30 por tamaño):"
          find tmp/unzipped -type f -iname "*.csv" -print0 \
            | xargs -0 -I{} bash -c 'echo -n "{} | "; stat -c "%s bytes" "{}"; echo -n " | "; wc -l "{}"' \
            | sort -nrk3 | head -n 30 || true

      # Paso 4: Normalizar headers (solo header) + guardar en data/ + cargar a BigQuery como tablas nombre_YYYY_MM
      - name: Normalizar headers y cargar CSVs a BigQuery (tablas por mes) + verificación
        shell: bash
        run: |
          set -euxo pipefail

          python - <<'PY'
          import os, re, glob, subprocess, sys

          PROJECT_ID = os.environ["PROJECT_ID"]
          DATASET     = os.environ["DATASET"]
          YEAR        = os.environ["YEAR"]
          MONTH       = os.environ["MONTH"]
          SUFFIX      = f"{YEAR}_{MONTH}"

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              return r.returncode, r.stdout

          def bq_count(dataset_dot_table: str) -> int:
              sql = f"SELECT COUNT(1) AS c FROM `{PROJECT_ID}.{dataset_dot_table}`"
              cmd = ["bq", f"--project_id={PROJECT_ID}", "query", "--use_legacy_sql=false", "--format=csv", sql]
              code, out = run(cmd)
              if code != 0:
                  print(f"[WARN] No pude contar {dataset_dot_table}:\n{out}")
                  return -1
              lines = [ln.strip() for ln in out.splitlines() if ln.strip()]
              return int(lines[1]) if len(lines) >= 2 else -1

          def normalize_headers_line(header_text: str) -> str:
              cols = header_text.rstrip("\n\r").split(",")
              fixed = []
              seen = {}
              for c in cols:
                  c = c.strip()
                  c = c.replace("/", "_").replace(" ", "_").replace("-", "_")
                  c = re.sub(r"[^A-Za-z0-9_]", "_", c)
                  c = re.sub(r"_+", "_", c).strip("_")
                  if not c:
                      c = "col"
                  if c[0].isdigit():
                      c = "c_" + c

                  base = c
                  k = seen.get(base, 0)
                  if k > 0:
                      c = f"{base}_{k}"
                  seen[base] = k + 1
                  fixed.append(c)
              return ",".join(fixed) + "\n"

          csv_paths = sorted(glob.glob("tmp/unzipped/**/*.csv", recursive=True))
          if not csv_paths:
              print("No se encontraron CSVs para procesar.")
              sys.exit(1)

          os.makedirs("data", exist_ok=True)

          created_tables = []

          for src in csv_paths:
              base = os.path.basename(src)              # records.csv
              name_no_ext = os.path.splitext(base)[0]   # records

              table_base = re.sub(r"[^A-Za-z0-9_]", "_", name_no_ext)
              table_base = re.sub(r"_+", "_", table_base).strip("_") or "tabla"
              if table_base[0].isdigit():
                  table_base = "t_" + table_base

              table = f"{table_base}_{SUFFIX}"          # records_2026_01
              out_clean = os.path.join("data", f"{table}.csv")

              # Contar filas en origen (rápido con wc -l)
              code, out = run(["bash", "-lc", f"wc -l '{src}' | awk '{{print $1}}'"])
              src_lines = int(out.strip()) if code == 0 and out.strip().isdigit() else -1
              if src_lines <= 1:
                  print(f"[SKIP] {base}: parece vacío o solo header (wc -l={src_lines})")
                  continue

              # Leer binario completo, quitar NUL (ASCII 0)
              with open(src, "rb") as f:
                  b = f.read().replace(b"\x00", b"")

              # Separar SOLO primera línea (header) del resto SIN reinterpretar el resto
              # Buscamos primer salto de línea.
              nl_pos = b.find(b"\n")
              if nl_pos == -1:
                  print(f"[SKIP] {base}: no encontré salto de línea en header")
                  continue

              header_bytes = b[:nl_pos+1]
              rest_bytes   = b[nl_pos+1:]

              # Decodificar header tolerante (solo header)
              header_text = header_bytes.decode("utf-8", errors="replace")
              header_norm = normalize_headers_line(header_text)

              # Escribir: header normalizado + resto bytes como texto "lo más seguro posible"
              # Para BigQuery, conviene UTF-8. Si hay bytes raros, se reemplazan.
              # Convertimos resto a texto con replace SIN tocar saltos de línea ya existentes.
              rest_text = rest_bytes.decode("utf-8", errors="replace")

              with open(out_clean, "w", encoding="utf-8", newline="") as g:
                  g.write(header_norm)
                  g.write(rest_text)

              # Verificar filas del archivo limpio (wc -l)
              code, out = run(["bash", "-lc", f"wc -l '{out_clean}' | awk '{{print $1}}'"])
              clean_lines = int(out.strip()) if code == 0 and out.strip().isdigit() else -1

              full_table = f"{DATASET}.{table}"

              # Cargar a BigQuery (REPLACE)
              # IMPORTANTE: max_bad_records=0 para que NO te “pierda filas” en silencio.
              cmd = [
                  "bq", f"--project_id={PROJECT_ID}", "load",
                  "--source_format=CSV",
                  "--field_delimiter=,",
                  "--quote=\"",
                  "--encoding=UTF-8",
                  "--autodetect",
                  "--skip_leading_rows=1",
                  "--allow_quoted_newlines",
                  "--allow_jagged_rows",
                  "--max_bad_records=0",
                  "--replace",
                  full_table,
                  out_clean
              ]

              print(f"\n=== Cargando {base} -> {PROJECT_ID}:{full_table} ===")
              print(f"Filas src (wc -l): {src_lines} | Filas clean (wc -l): {clean_lines}")
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              print(r.stdout)

              if r.returncode != 0:
                  print(f"[ERROR] Falló carga de {base} -> {full_table}")
                  sys.exit(r.returncode)

              # Verificación filas en BigQuery
              bq_rows = bq_count(full_table)
              expected_rows = src_lines - 1  # sin header
              print(f"BigQuery rows: {bq_rows} | Esperadas aprox: {expected_rows}")

              if bq_rows != expected_rows:
                  print(f"[ERROR] Mismatch filas en {full_table}. BQ={bq_rows}, CSV={expected_rows}")
                  sys.exit(2)

              created_tables.append(full_table)

          with open("tmp/created_tables.txt", "w", encoding="utf-8") as f:
              for t in created_tables:
                  f.write(t + "\n")

          print("\nOK: Tablas mensuales cargadas y verificadas (filas exactas).")
          PY

      # Paso 5: Crear/Actualizar VIEWS por familia (records, releases, etc.) usando UNION ALL
      # OJO: si cargas solo 1 mes, la view 'records' debería tener lo mismo que records_YYYY_MM
      - name: Crear/Actualizar views (unión por familia)
        shell: bash
        run: |
          set -euxo pipefail

          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false "
          DECLARE project_id STRING DEFAULT '${PROJECT_ID}';
          DECLARE dataset_id STRING DEFAULT '${DATASET}';

          FOR fam IN (
            SELECT DISTINCT REGEXP_REPLACE(table_name, r'_[0-9]{4}_[0-9]{2}$', '') AS base
            FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
            WHERE table_type = 'BASE TABLE'
              AND REGEXP_CONTAINS(table_name, r'_[0-9]{4}_[0-9]{2}$')
          ) DO

            DECLARE base_name STRING DEFAULT fam.base;
            DECLARE tables ARRAY<STRING>;
            DECLARE union_sql STRING;

            SET tables = (
              SELECT ARRAY_AGG(table_name ORDER BY table_name)
              FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
              WHERE table_type = 'BASE TABLE'
                AND REGEXP_CONTAINS(table_name, FORMAT(r'^%s_[0-9]{4}_[0-9]{2}$', base_name))
            );

            IF ARRAY_LENGTH(tables) IS NULL OR ARRAY_LENGTH(tables) = 0 THEN
              CONTINUE;
            END IF;

            -- Para 1 mes: selecciona todo de la única tabla.
            IF ARRAY_LENGTH(tables) = 1 THEN
              EXECUTE IMMEDIATE FORMAT(
                'CREATE OR REPLACE VIEW `%s.%s.%s` AS SELECT * FROM `%s.%s.%s`',
                project_id, dataset_id, base_name,
                project_id, dataset_id, tables[OFFSET(0)]
              );
            ELSE
              -- Para multi-mes: esta versión simple asume schema igual.
              -- (Luego lo refinamos a "solo columnas comunes" si lo necesitas sí o sí)
              SET union_sql = (
                SELECT STRING_AGG(
                  FORMAT('SELECT * FROM `%s.%s.%s`', project_id, dataset_id, t),
                  ' UNION ALL '
                )
                FROM UNNEST(tables) AS t
              );

              EXECUTE IMMEDIATE FORMAT(
                'CREATE OR REPLACE VIEW `%s.%s.%s` AS %s',
                project_id, dataset_id, base_name, union_sql
              );
            END IF;

          END FOR;
          "

      # Paso 6: Verificación rápida: si existe view records, contar
      - name: Verificar view records (si existe)
        shell: bash
        run: |
          set -euxo pipefail
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false \
            "SELECT COUNT(1) AS rows FROM \`${PROJECT_ID}.${DATASET}.records\`" \
            --format=prettyjson || true

      # Paso 7: Ver data/ (solo para depurar)
      - name: Ver data/
        shell: bash
        run: |
          set -euxo pipefail
          ls -lah data || true
