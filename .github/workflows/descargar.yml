name: SEACE -> BigQuery (STRING + staging mensual + UNION final + lineage)

on:
  workflow_dispatch: {}

concurrency:
  group: seace-bq-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  seace_union_bq:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      FINAL_DS: github_actions
      STG_DS: github_actions_stg

      MONTHS: "2026-01,2025-12,2025-11"

      # 22 fijas
      EXPECTED_FINAL_TABLES: "com_awa_ite_additionalClassific,com_awa_ite_tot_exchangeRates,com_awa_items,com_awa_suppliers,com_awa_val_exchangeRates,com_awards,com_con_documents,com_con_ite_additionalClassific,com_con_ite_tot_exchangeRates,com_con_items,com_con_val_exchangeRates,com_contracts,com_par_additionalIdentifiers,com_parties,com_sources,com_ten_documents,com_ten_ite_additionalClassific,com_ten_ite_tot_exchangeRates,com_ten_items,com_ten_tenderers,records,releases"

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}

      - name: Limpieza local inicial
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p tmp/zips tmp/unzipped data reports
          rm -rf tmp/zips/* tmp/unzipped/* data/* reports/* || true

      - name: Limpieza BigQuery (FINAL y STG) antes de cargar (HARD, solo objetos) - via API
        shell: bash
        run: |
          set -euo pipefail

          # Asegura datasets (NO se borran)
          bq --quiet --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$FINAL_DS" >/dev/null 2>&1 || true
          bq --quiet --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$STG_DS"   >/dev/null 2>&1 || true

          # Instala cliente oficial para no depender del CLI bq ls
          python -m pip install --quiet --disable-pip-version-check google-cloud-bigquery

          python -u - <<'PY'
          import os, sys, time
          from google.cloud import bigquery
          from google.api_core.exceptions import NotFound, TooManyRequests, ServiceUnavailable, Forbidden, BadRequest, Conflict

          PROJECT_ID = os.environ["PROJECT_ID"]
          FINAL_DS   = os.environ["FINAL_DS"]
          STG_DS     = os.environ["STG_DS"]

          client = bigquery.Client(project=PROJECT_ID)

          def retry(fn, what: str, max_retries: int = 8):
              """
              Retry (ritrái) con backoff (bák-óf) exponencial para errores transitorios.
              """
              for i in range(max_retries):
                  try:
                      return fn()
                  except (TooManyRequests, ServiceUnavailable, Conflict) as e:
                      sleep_s = 2 ** i
                      print(f"[WARN] {what} falló por {type(e).__name__}. Retry en {sleep_s}s...", flush=True)
                      time.sleep(sleep_s)
              # último intento sin esconder el error
              return fn()

          def wipe_dataset_objects(ds: str):
              print(f"\n=== LIMPIANDO DATASET {PROJECT_ID}:{ds} (solo objetos) ===", flush=True)
              dataset_ref = bigquery.DatasetReference(PROJECT_ID, ds)

              # 1) RUTINAS (routines)
              routines = []
              try:
                  routines = list(client.list_routines(dataset_ref))
              except Exception as e:
                  print(f"[WARN] No se pudo listar routines en {ds}: {e}", flush=True)

              # 2) MODELOS (models)
              models = []
              try:
                  models = list(client.list_models(dataset_ref))
              except Exception as e:
                  print(f"[WARN] No se pudo listar models en {ds}: {e}", flush=True)

              # 3) TABLAS/VISTAS (tables/views/materialized views)
              tables = []
              try:
                  tables = list(client.list_tables(dataset_ref))
              except NotFound:
                  # Si el dataset no existe, lo tratamos como vacío (pero arriba lo “mk” ya lo asegura)
                  print(f"[WARN] Dataset {ds} no existe al listar tables. Se asume vacío.", flush=True)
                  return
              except Exception as e:
                  print(f"[ERROR] No se pudo listar tables en {ds}: {e}", flush=True)
                  sys.exit(1)

              print(f"Encontrados -> routines: {len(routines)} | models: {len(models)} | tables/views: {len(tables)}", flush=True)

              # BORRADO: primero routines, luego models, luego tables
              # (Orden defensivo por dependencias raras)

              for r in routines:
                  rid = getattr(r, "routine_id", None) or getattr(r, "routineId", None) or str(r)
                  def _del_r():
                      return client.delete_routine(r.reference)  # type: ignore
                  try:
                      retry(_del_r, f"delete routine {ds}.{rid}")
                      print(f"Borrado routine: {ds}.{rid}", flush=True)
                  except NotFound:
                      pass
                  except (Forbidden, BadRequest) as e:
                      print(f"[ERROR] No se pudo borrar routine {ds}.{rid}: {e}", flush=True)
                      sys.exit(1)

              for m in models:
                  mid = getattr(m, "model_id", None) or getattr(m, "modelId", None) or str(m)
                  def _del_m():
                      return client.delete_model(m.reference)  # type: ignore
                  try:
                      retry(_del_m, f"delete model {ds}.{mid}")
                      print(f"Borrado model: {ds}.{mid}", flush=True)
                  except NotFound:
                      pass
                  except (Forbidden, BadRequest) as e:
                      print(f"[ERROR] No se pudo borrar model {ds}.{mid}: {e}", flush=True)
                      sys.exit(1)

              for t in tables:
                  tid = t.table_id
                  def _del_t():
                      return client.delete_table(t.reference, not_found_ok=True)  # type: ignore
                  try:
                      retry(_del_t, f"delete table {ds}.{tid}")
                      print(f"Borrado table: {ds}.{tid}", flush=True)
                  except (Forbidden, BadRequest) as e:
                      print(f"[ERROR] No se pudo borrar table {ds}.{tid}: {e}", flush=True)
                      sys.exit(1)

              # Verificación final (re-lista)
              tables2 = list(client.list_tables(dataset_ref))
              models2 = []
              routines2 = []
              try:
                  models2 = list(client.list_models(dataset_ref))
              except Exception:
                  pass
              try:
                  routines2 = list(client.list_routines(dataset_ref))
              except Exception:
                  pass

              if tables2 or models2 or routines2:
                  print(f"[ERROR] Limpieza incompleta en {ds}. Restan -> routines:{len(routines2)} models:{len(models2)} tables:{len(tables2)}", flush=True)
                  if tables2:
                      for x in tables2[:50]:
                          print(f"  - {ds}.{x.table_id} (table/view)", flush=True)
                  if models2:
                      for x in models2[:50]:
                          mid = getattr(x, "model_id", None) or str(x)
                          print(f"  - {ds}.{mid} (model)", flush=True)
                  if routines2:
                      for x in routines2[:50]:
                          rid = getattr(x, "routine_id", None) or str(x)
                          print(f"  - {ds}.{rid} (routine)", flush=True)
                  sys.exit(1)

              print(f"OK: {ds} quedó vacío.", flush=True)

          wipe_dataset_objects(FINAL_DS)
          wipe_dataset_objects(STG_DS)
          print("\nOK: Limpieza BigQuery completada (sin borrar datasets).", flush=True)
          PY

      - name: Descargar ZIPs SEACE
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p tmp/zips

          IFS=',' read -r -a MONTH_LIST <<< "$MONTHS"
          test "${#MONTH_LIST[@]}" -gt 0

          for ym in "${MONTH_LIST[@]}"; do
            ym="$(echo "$ym" | xargs)"
            [[ "$ym" =~ ^[0-9]{4}-[0-9]{2}$ ]]

            YEAR="${ym%-*}"
            MONTH="${ym#*-}"

            URL="https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/${YEAR}/${MONTH}"
            ZIP="tmp/zips/seace_${YEAR}_${MONTH}.zip"

            echo "Descargando $ym -> $ZIP"
            curl -L -f -sS --retry 8 --retry-all-errors --retry-delay 2 "$URL" -o "$ZIP"
            test -s "$ZIP"
          done

          echo "ZIPs descargados:"
          ls -lah tmp/zips
          COUNT=$(ls -1 tmp/zips/*.zip 2>/dev/null | wc -l)
          echo "ZIP count: $COUNT"
          test "$COUNT" -gt 0

      - name: STAGING + FINAL (22 fijas) + lineage cacheado
        shell: bash
        run: |
          set -euo pipefail

          python -u - <<'PY'
          import os, re, glob, shutil, subprocess, sys, json, time
          from collections import defaultdict

          PROJECT_ID = os.environ["PROJECT_ID"]
          FINAL_DS   = os.environ["FINAL_DS"]
          STG_DS     = os.environ["STG_DS"]
          EXPECTED = [x.strip() for x in os.environ["EXPECTED_FINAL_TABLES"].split(",") if x.strip()]
          if len(EXPECTED) != 22:
              print(f"[ERROR] EXPECTED_FINAL_TABLES debe tener 22 nombres. Ahora tiene: {len(EXPECTED)}", flush=True)
              sys.exit(1)

          ZIPS_DIR   = "tmp/zips"
          UNZIP_DIR  = "tmp/unzipped"
          DATA_DIR   = "data"
          REPORT_DIR = "reports"
          os.makedirs(UNZIP_DIR, exist_ok=True)
          os.makedirs(DATA_DIR, exist_ok=True)
          os.makedirs(REPORT_DIR, exist_ok=True)

          def sh(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              out = r.stdout or ""
              print(out, flush=True)
              return r.returncode, out

          def sh_retry(cmd, tries=6):
              last = (1, "")
              for i in range(tries):
                  rc, out = sh(cmd)
                  last = (rc, out)
                  if rc == 0:
                      return 0, out
                  time.sleep(2 ** i)
              return last

          def safe_table_name(name_no_ext: str) -> str:
              t = re.sub(r"[^A-Za-z0-9_]", "_", name_no_ext)
              t = re.sub(r"_+", "_", t).strip("_")
              if not t:
                  t = "tabla"
              if t[0].isdigit():
                  t = "t_" + t
              return t

          def normalize_headers_line(header: str):
              cols = header.rstrip("\n\r").split(",")
              fixed = []
              seen = {}
              for c in cols:
                  c = c.strip()
                  c = c.replace("/", "_").replace(" ", "_").replace("-", "_")
                  c = re.sub(r"[^A-Za-z0-9_]", "_", c)
                  c = re.sub(r"_+", "_", c).strip("_")
                  if not c:
                      c = "col"
                  if c[0].isdigit():
                      c = "c_" + c
                  base = c
                  k = seen.get(base, 0)
                  if k > 0:
                      c = f"{base}_{k}"
                  seen[base] = k + 1
                  fixed.append(c)
              return fixed

          def bq_common_columns(ds: str, tables: list[str]) -> list[str]:
              if not tables:
                  return []
              table_names_sql = ", ".join([f"'{t}'" for t in tables])
              sql = f"""
              SELECT column_name
              FROM `{PROJECT_ID}.{ds}.INFORMATION_SCHEMA.COLUMNS`
              WHERE table_name IN ({table_names_sql})
              GROUP BY column_name
              HAVING COUNT(DISTINCT table_name) = {len(tables)}
              ORDER BY column_name
              """
              rc, out = sh_retry(["bq", "--quiet", f"--project_id={PROJECT_ID}", "query", "--nouse_legacy_sql", "--format=csv", sql], tries=6)
              if rc != 0:
                  return []
              rows = [r.strip() for r in out.splitlines() if r.strip()]
              if len(rows) >= 2 and rows[0] == "column_name":
                  return rows[1:]
              return []

          def bq_list_tables(ds: str) -> list[str]:
              rc, out = sh_retry(["bq", "--quiet", f"--project_id={PROJECT_ID}", "ls", "--max_results", "5000", "--format=prettyjson", f"{PROJECT_ID}:{ds}"], tries=4)
              if rc != 0:
                  return []
              # En esta parte solo lo usamos para verificar finales (debería salir estable).
              try:
                  items = json.loads(out)
              except Exception:
                  return []
              names = []
              if isinstance(items, list):
                  for it in items:
                      ref = (it or {}).get("tableReference") or {}
                      tid = ref.get("tableId")
                      if tid:
                          names.append(tid)
              return sorted(set(names))

          zip_files = sorted(glob.glob(os.path.join(ZIPS_DIR, "seace_????_??.zip")))
          if not zip_files:
              print("[ERROR] No hay ZIPs en tmp/zips.", flush=True)
              sys.exit(1)

          stg_tables_by_logical = defaultdict(list)

          print("\n==============================", flush=True)
          print("FASE 1: STAGING (TODO STRING)", flush=True)
          print("==============================", flush=True)

          for zip_path in zip_files:
              m = re.search(r"seace_(\d{4})_(\d{2})\.zip$", os.path.basename(zip_path))
              if not m:
                  continue

              year, month = m.group(1), m.group(2)
              ym = f"{year}-{month}"

              shutil.rmtree(UNZIP_DIR, ignore_errors=True)
              shutil.rmtree(DATA_DIR, ignore_errors=True)
              os.makedirs(UNZIP_DIR, exist_ok=True)
              os.makedirs(DATA_DIR, exist_ok=True)

              print(f"\n--- Mes {ym}: unzip ---", flush=True)
              rc, _ = sh_retry(["unzip", "-o", zip_path, "-d", UNZIP_DIR], tries=3)
              if rc != 0:
                  sys.exit(rc)

              csv_paths = sorted(glob.glob(os.path.join(UNZIP_DIR, "*.csv")))
              print(f"CSVs encontrados en {ym}: {len(csv_paths)}", flush=True)

              for src in csv_paths:
                  base = os.path.basename(src)
                  logical = safe_table_name(os.path.splitext(base)[0])

                  stg_table = f"{logical}__{year}_{month}"
                  stg_full  = f"{STG_DS}.{stg_table}"

                  out_csv     = os.path.join(DATA_DIR, f"{stg_table}.csv")
                  schema_path = os.path.join(DATA_DIR, f"{stg_table}_schema.json")

                  with open(src, "rb") as f:
                      b = f.read().replace(b"\x00", b"")
                  text = b.decode("utf-8", errors="replace").splitlines(True)
                  if not text:
                      continue

                  cols = normalize_headers_line(text[0])
                  text[0] = ",".join(cols) + "\n"

                  with open(out_csv, "w", encoding="utf-8", newline="") as g:
                      g.writelines(text)

                  schema = [{"name": c, "type": "STRING", "mode": "NULLABLE"} for c in cols]
                  with open(schema_path, "w", encoding="utf-8") as s:
                      json.dump(schema, s)

                  cmd = [
                      "bq", "--quiet", f"--project_id={PROJECT_ID}", "load",
                      "--source_format=CSV",
                      "--skip_leading_rows=1",
                      "--allow_quoted_newlines",
                      "--allow_jagged_rows",
                      "--max_bad_records=5000",
                      "--replace",
                      f"--schema={schema_path}",
                      stg_full,
                      out_csv
                  ]
                  print(f"STG load: {ym} | {base} -> {PROJECT_ID}:{stg_full}", flush=True)
                  rc, _ = sh_retry(cmd, tries=6)
                  if rc != 0:
                      print(f"[ERROR] Falló staging load: {stg_full}", flush=True)
                      sys.exit(1)

                  stg_tables_by_logical[logical].append(stg_table)

          print("\n==============================", flush=True)
          print("FASE 2: 22 TABLAS FINALES (CATÁLOGO FIJO) + LINEAGE", flush=True)
          print("==============================", flush=True)

          lineage = []

          for logical in EXPECTED:
              stg_list = sorted([t for t in stg_tables_by_logical.get(logical, []) if t])

              if not stg_list:
                  sql = f"""
                  CREATE OR REPLACE TABLE `{PROJECT_ID}.{FINAL_DS}.{logical}` AS
                  SELECT CAST(NULL AS STRING) AS anio_mes
                  WHERE 1=0
                  """
                  rc, _ = sh_retry(["bq", "--quiet", f"--project_id={PROJECT_ID}", "query", "--nouse_legacy_sql", sql], tries=6)
                  if rc != 0:
                      print(f"[ERROR] No se pudo crear tabla vacía: {FINAL_DS}.{logical}", flush=True)
                      sys.exit(1)

                  lineage.append({
                      "final_table": f"{FINAL_DS}.{logical}",
                      "staging_tables_used": [],
                      "months_used": [],
                      "created_empty": True
                  })
                  continue

              common_cols = bq_common_columns(STG_DS, stg_list)

              union_parts = []
              months_used = []
              for t in stg_list:
                  mm = re.search(r"__(\d{4})_(\d{2})$", t)
                  if not mm:
                      continue
                  ym = f"{mm.group(1)}-{mm.group(2)}"
                  months_used.append(ym)

                  exprs = [f"'{ym}' AS anio_mes"]
                  for c in common_cols:
                      if c == "anio_mes":
                          continue
                      exprs.append(f"CAST({c} AS STRING) AS {c}")

                  union_parts.append(
                      "SELECT " + ", ".join(exprs) + f" FROM `{PROJECT_ID}.{STG_DS}.{t}`"
                  )

              if not union_parts:
                  print(f"[ERROR] No hay UNION parts para {logical}", flush=True)
                  sys.exit(1)

              final_sql = f"""
              CREATE OR REPLACE TABLE `{PROJECT_ID}.{FINAL_DS}.{logical}` AS
              {("\nUNION ALL\n").join(union_parts)}
              """

              rc, _ = sh_retry(["bq", "--quiet", f"--project_id={PROJECT_ID}", "query", "--nouse_legacy_sql", final_sql], tries=6)
              if rc != 0:
                  print(f"[ERROR] crear final falló para {logical}", flush=True)
                  sys.exit(1)

              lineage.append({
                  "final_table": f"{FINAL_DS}.{logical}",
                  "staging_tables_used": [f"{STG_DS}.{x}" for x in stg_list],
                  "months_used": sorted(list(set(months_used))),
                  "created_empty": False
              })

          out_path = os.path.join(REPORT_DIR, "final_lineage.json")
          with open(out_path, "w", encoding="utf-8") as f:
              json.dump(lineage, f, ensure_ascii=False, indent=2)
          print(f"\nOK: Lineage guardado en {out_path}", flush=True)

          print("\n==============================", flush=True)
          print("FASE 3: VERIFICACIÓN FINAL (22 EXACTAS)", flush=True)
          print("==============================", flush=True)

          actual = bq_list_tables(FINAL_DS)
          expected_set = set(EXPECTED)
          actual_set = set(actual)

          missing = sorted(list(expected_set - actual_set))
          extra = sorted(list(actual_set - expected_set))

          print(f"Esperadas: {len(EXPECTED)} | Reales: {len(actual)}", flush=True)

          if missing:
              print("[ERROR] FALTAN:", flush=True)
              for t in missing:
                  print(f"  - {FINAL_DS}.{t}", flush=True)
              sys.exit(1)

          if extra:
              print("[ERROR] SOBRAN (basura):", flush=True)
              for t in extra:
                  print(f"  - {FINAL_DS}.{t}", flush=True)
              sys.exit(1)

          print("OK: están las 22 tablas finales. Sin extras.", flush=True)

          print("\n==============================", flush=True)
          print("FASE 4: LIMPIEZA STAGING (DROP)", flush=True)
          print("==============================", flush=True)

          dropped = 0
          for logical, stg_list in stg_tables_by_logical.items():
              for t in stg_list:
                  ref = f"{PROJECT_ID}:{STG_DS}.{t}"
                  rc, _ = sh_retry(["bq", "--quiet", f"--project_id={PROJECT_ID}", "rm", "-f", "-t", ref], tries=6)
                  if rc == 0:
                      dropped += 1

          print(f"Staging dropeadas: {dropped}", flush=True)
          PY

      - name: Ver lineage cacheado
        shell: bash
        run: |
          set -euo pipefail
          ls -lah reports
          echo "---- final_lineage.json ----"
          cat reports/final_lineage.json
