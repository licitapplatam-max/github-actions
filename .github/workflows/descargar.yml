name: SEACE CSV ZIP -> BigQuery (3 meses FULL CLEAN + carga + unión estable)

on:
  workflow_dispatch: {}

jobs:
  seace_3months_full:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      MONTHS: "2025-11 2025-12 2026-01"
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth Google
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # =========================
      # 1️⃣ CREAR DATASET SI NO EXISTE
      # =========================
      - name: Crear dataset si no existe
        shell: bash
        run: |
          set -e
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" || true

      # =========================
      # 2️⃣ LIMPIEZA TOTAL BIGQUERY (TODO)
      # =========================
      - name: LIMPIEZA TOTAL BigQuery (borra TODAS las tablas del dataset)
        shell: bash
        run: |
          set -e
          tables=$(bq --project_id="$PROJECT_ID" ls "$DATASET" | awk 'NR>2 {print $1}')
          for t in $tables; do
            echo "Borrando $DATASET.$t"
            bq --project_id="$PROJECT_ID" rm -f -t "$DATASET.$t"
          done
          echo "Dataset completamente limpio."

      # =========================
      # 3️⃣ LIMPIEZA RUNNER
      # =========================
      - name: Limpiar runner
        shell: bash
        run: |
          rm -rf data tmp
          mkdir -p data tmp

      # =========================
      # 4️⃣ DESCARGA + CONTEO REAL
      # =========================
      - name: Descargar ZIPs y contar filas reales
        shell: bash
        run: |
          python - <<'PY'
          import os, zipfile
          from pathlib import Path
          from collections import defaultdict
          import subprocess

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp"); tmp.mkdir(exist_ok=True)

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              if r.returncode != 0:
                  print(r.stdout)
                  raise SystemExit(1)

          def count_lines(p):
              with p.open("rb") as f:
                  return sum(1 for _ in f)

          expected = []

          for ym in MONTHS:
              y,m = ym.split("-")
              url = f"https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/{y}/{m}"
              zipf = tmp / f"{y}_{m}.zip"
              out  = tmp / f"unzipped_{y}_{m}"

              run(["bash","-lc", f"curl -L -f '{url}' -o '{zipf}'"])
              out.mkdir(exist_ok=True)

              with zipfile.ZipFile(zipf) as z:
                  z.extractall(out)

              for csv in out.glob("*.csv"):
                  rows = max(count_lines(csv)-1,0)
                  fam = csv.stem
                  expected.append(f"{fam}\t{ym}\t{rows}")

          (tmp/"expected.tsv").write_text("family\tym\texpected\n"+"\n".join(expected))
          print("Conteo real listo.")
          PY

      # =========================
      # 5️⃣ NORMALIZAR CSVs (TODO STRING)
      # =========================
      - name: Normalizar CSVs
        shell: bash
        run: |
          python - <<'PY'
          import re, json
          from pathlib import Path

          tmp = Path("tmp")
          data = Path("data"); data.mkdir(exist_ok=True)

          def norm(s):
              s = re.sub(r'[^A-Za-z0-9_]', '_', s.strip())
              return s if not s[0].isdigit() else "c_"+s

          for d in tmp.glob("unzipped_*"):
              y,m = d.name.split("_")[1:]
              for f in d.glob("*.csv"):
                  base = norm(f.stem)
                  table = f"{base}_{y}_{m}"
                  lines = f.read_text(errors="replace").splitlines()
                  hdr = [norm(c) for c in lines[0].split(",")]
                  lines[0] = ",".join(hdr)
                  (data/f"{table}.csv").write_text("\n".join(lines))
                  schema=[{"name":c,"type":"STRING","mode":"NULLABLE"} for c in hdr]
                  (data/f"{table}.schema.json").write_text(json.dumps(schema))
          print("CSVs normalizados.")
          PY

      # =========================
      # 6️⃣ CARGA A BIGQUERY
      # =========================
      - name: Cargar CSVs a BigQuery
        shell: bash
        run: |
          for csv in data/*.csv; do
            table=$(basename "$csv" .csv)
            bq --project_id="$PROJECT_ID" load \
              --replace \
              --source_format=CSV \
              --skip_leading_rows=1 \
              --allow_quoted_newlines \
              "$DATASET.$table" \
              "$csv" \
              "data/$table.schema.json"
          done

      # =========================
      # 7️⃣ UNIÓN POR FAMILIA (SQL PLANO)
      # =========================
      - name: Unir tablas por familia (estable)
        shell: bash
        run: |
          python - <<'PY'
          import subprocess, re
          from collections import defaultdict

          PROJECT=os.environ["PROJECT_ID"]
          DATASET=os.environ["DATASET"]

          out=subprocess.check_output(
              ["bq","--project_id",PROJECT,"ls",DATASET],
              text=True
          )

          tables=[l.split()[0] for l in out.splitlines()[2:]]
          fams=defaultdict(list)

          for t in tables:
              m=re.search(r'(.+)_\d{4}_\d{2}$',t)
              if m: fams[m.group(1)].append(t)

          for fam,ts in fams.items():
              parts=[]
              for t in ts:
                  parts.append(f"SELECT * FROM `{PROJECT}.{DATASET}.{t}`")
              sql=" UNION ALL ".join(parts)
              q=f"CREATE OR REPLACE TABLE `{PROJECT}.{DATASET}.{fam}` AS {sql}"
              subprocess.check_call(["bq","--project_id",PROJECT,"query","--use_legacy_sql=false",q])

          print("Unión completa.")
          PY
