name: SEACE CSV ZIP -> BigQuery (3 meses FULL CLEAN + carga + union DEFINITIVA)

on:
  workflow_dispatch: {}

jobs:
  seace_3months_full:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      MONTHS: "2025-11 2025-12 2026-01"
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth Google
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # 0) Crear dataset si no existe
      - name: Crear dataset si no existe
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" || true

      # 1) LIMPIEZA TOTAL REAL usando INFORMATION_SCHEMA
      - name: LIMPIEZA TOTAL BigQuery (REAL)
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false "
          DECLARE tbl STRING;
          FOR r IN (
            SELECT table_name
            FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
          ) DO
            SET tbl = r.table_name;
            EXECUTE IMMEDIATE FORMAT(
              'DROP TABLE IF EXISTS `%s.%s.%s`',
              '${PROJECT_ID}', '${DATASET}', tbl
            );
          END FOR;
          "

      # 2) Limpieza runner
      - name: Limpiar runner
        shell: bash
        run: |
          rm -rf data tmp || true
          mkdir -p data tmp

      # 3) Descargar ZIPs + extraer + conteo REAL
      - name: Descargar ZIPs + extraer + conteo REAL
        shell: bash
        run: |
          set -euo pipefail
          python <<'PY'
          import os, zipfile, subprocess
          from pathlib import Path
          from collections import defaultdict

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp"); tmp.mkdir(exist_ok=True)

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              if r.returncode != 0:
                  print(r.stdout)
                  raise SystemExit(r.returncode)

          def count_rows(p):
              with p.open("rb") as f:
                  return max(sum(1 for _ in f) - 1, 0)

          for ym in MONTHS:
              y, m = ym.split("-")
              url = f"https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/{y}/{m}"
              zip_path = tmp / f"seace_{y}_{m}.zip"
              out_dir = tmp / f"unzipped_{y}_{m}"

              run(["bash","-lc", f"curl -L -f -sS '{url}' -o '{zip_path}'"])
              out_dir.mkdir(parents=True, exist_ok=True)
              with zipfile.ZipFile(zip_path) as z:
                  z.extractall(out_dir)

              print(f"\n=== {ym} ===")
              for f in sorted(out_dir.glob("*.csv")):
                  print(f"{f.name:40s} rows={count_rows(f):10d}")
          PY

      # 4) Normalizar CSVs + schema TODO STRING
      - name: Normalizar CSVs
        shell: bash
        run: |
          python <<'PY'
          import os, re, json
          from pathlib import Path

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp")
          data = Path("data"); data.mkdir(exist_ok=True)

          def norm(s):
              s = re.sub(r"[^A-Za-z0-9_]", "_", s.strip())
              s = re.sub(r"_+", "_", s).strip("_")
              return s if not s[0].isdigit() else "c_" + s

          for ym in MONTHS:
              y,m = ym.split("-")
              for src in (tmp / f"unzipped_{y}_{m}").glob("*.csv"):
                  lines = src.read_text(errors="replace").splitlines(True)
                  hdr = [norm(c) for c in lines[0].split(",")]
                  lines[0] = ",".join(hdr) + "\n"

                  name = f"{norm(src.stem)}_{y}_{m}"
                  (data / f"{name}.csv").write_text("".join(lines))
                  (data / f"{name}.schema.json").write_text(
                      json.dumps([{"name":c,"type":"STRING"} for c in hdr])
                  )
          PY

      # 5) Cargar CSVs
      - name: Cargar CSVs
        shell: bash
        run: |
          for csv in data/*.csv; do
            t=$(basename "$csv" .csv)
            bq --project_id="$PROJECT_ID" load \
              --replace \
              --source_format=CSV \
              --skip_leading_rows=1 \
              --allow_quoted_newlines \
              "$DATASET.$t" "$csv" "data/$t.schema.json"
          done

      # 6) UNION DEFINITIVA usando INFORMATION_SCHEMA
      - name: Union DEFINITIVA por familia
        shell: bash
        run: |
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false "
          DECLARE fam STRING;
          FOR f IN (
            SELECT DISTINCT REGEXP_REPLACE(table_name, r'_[0-9]{4}_[0-9]{2}$','') fam
            FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
            WHERE REGEXP_CONTAINS(table_name, r'_[0-9]{4}_[0-9]{2}$')
          ) DO
            EXECUTE IMMEDIATE (
              SELECT 'CREATE OR REPLACE TABLE `${PROJECT_ID}.${DATASET}.'||fam||'` AS '||
              STRING_AGG(
                'SELECT * FROM `${PROJECT_ID}.${DATASET}.'||table_name||'`',
                ' UNION ALL '
              )
              FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
              WHERE REGEXP_CONTAINS(table_name, '^'||fam||'_[0-9]{4}_[0-9]{2}$')
            );
          END FOR;
          "
