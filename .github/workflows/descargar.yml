name: SEACE CSV ZIP -> BigQuery (3 meses FULL CLEAN + carga + union estable)

on:
  workflow_dispatch: {}

jobs:
  seace_3months_full:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      MONTHS: "2025-11 2025-12 2026-01"
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth Google
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # 0) Crear dataset si no existe
      - name: Crear dataset si no existe
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" || true

      # 1) LIMPIEZA TOTAL BigQuery
      - name: LIMPIEZA TOTAL BigQuery (tablas + vistas)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, subprocess

          PROJECT = os.environ["PROJECT_ID"]
          DATASET = os.environ["DATASET"]

          out = subprocess.check_output(
              ["bq","--project_id",PROJECT,"ls",DATASET],
              text=True
          ).splitlines()

          names = [ln.split()[0] for ln in out[2:] if ln.strip()]

          for n in names:
              subprocess.call(
                  ["bq","--project_id",PROJECT,"rm","-f","-t",f"{DATASET}.{n}"]
              )

          print("OK: dataset limpio")
          PY

      # 2) Limpieza runner
      - name: Limpiar runner
        shell: bash
        run: |
          rm -rf data tmp || true
          mkdir -p data tmp

      # 3) Descargar ZIPs + conteo REAL
      - name: Descargar ZIPs + conteo REAL
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import zipfile, subprocess
          from pathlib import Path
          from collections import defaultdict
          import os

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp")

          def count(p):
              with p.open("rb") as f:
                  return max(sum(1 for _ in f) - 1, 0)

          for ym in MONTHS:
              y,m = ym.split("-")
              url = f"https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/{y}/{m}"
              z = tmp / f"{ym}.zip"
              d = tmp / ym
              subprocess.check_call(["curl","-L","-f","-sS",url,"-o",z])
              d.mkdir(exist_ok=True)
              with zipfile.ZipFile(z) as zz:
                  zz.extractall(d)

              print(f"\nMES {ym}")
              for f in sorted(d.glob("*.csv")):
                  print(f"{f.name:35s} rows={count(f)}")
          PY

      # 4) Normalizar CSV + schema STRING
      - name: Normalizar CSV
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import re, json, os
          from pathlib import Path

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp")
          data = Path("data")

          def norm(s):
              s = re.sub(r'[^A-Za-z0-9_]', '_', s.replace('/','_'))
              s = re.sub(r'_+', '_', s).strip('_')
              if s[0].isdigit(): s = 'c_' + s
              return s or 'col'

          for ym in MONTHS:
              y,m = ym.split("-")
              for src in (tmp/ym).glob("*.csv"):
                  name = f"{norm(src.stem)}_{y}_{m}"
                  b = src.read_bytes().replace(b'\x00',b'')
                  lines = b.decode("utf-8","replace").splitlines(True)
                  if not lines: continue
                  cols = []
                  seen = {}
                  for c in lines[0].split(","):
                      c = norm(c)
                      seen[c] = seen.get(c,0)+1
                      cols.append(c if seen[c]==1 else f"{c}_{seen[c]-1}")
                  lines[0] = ",".join(cols)+"\n"
                  (data/f"{name}.csv").write_text("".join(lines))
                  (data/f"{name}.schema.json").write_text(
                      json.dumps([{"name":c,"type":"STRING"} for c in cols])
                  )
          PY

      # 5) Cargar CSVs
      - name: Cargar CSVs
        shell: bash
        run: |
          set -euo pipefail
          for f in data/*.csv; do
            t=$(basename "$f" .csv)
            bq --project_id="$PROJECT_ID" load \
              --replace \
              --source_format=CSV \
              --skip_leading_rows=1 \
              "$DATASET.$t" "$f" "data/$t.schema.json"
          done

      # 6) UNION ESTABLE (fuente = data/)
      - name: UNION estable por familia
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os,re,subprocess
          from pathlib import Path
          from collections import defaultdict

          PROJECT=os.environ["PROJECT_ID"]
          DATASET=os.environ["DATASET"]

          fams=defaultdict(list)
          for f in Path("data").glob("*.csv"):
              t=f.stem
              if re.search(r'_\d{4}_\d{2}$',t):
                  fam=re.sub(r'_\d{4}_\d{2}$','',t)
                  fams[fam].append(t)

          def cols(t):
              q=f"""SELECT column_name FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.COLUMNS`
                    WHERE table_name='{t}' ORDER BY ordinal_position"""
              o=subprocess.check_output(
                  ["bq","--project_id",PROJECT,"query","--use_legacy_sql=false","--format=csv",q],
                  text=True)
              return [l for l in o.splitlines()[1:] if l]

          for fam,ts in fams.items():
              ts=sorted(ts)
              allc=[]
              seen=set()
              per={}
              for t in ts:
                  c=cols(t)
                  per[t]=set(c)
                  for x in c:
                      if x not in seen:
                          seen.add(x); allc.append(x)
              sels=[]
              for t in ts:
                  sels.append("SELECT "+", ".join(
                      f"`{c}`" if c in per[t] else f"CAST(NULL AS STRING) AS `{c}`"
                      for c in allc
                  )+f" FROM `{PROJECT}.{DATASET}.{t}`")
              sql=f"CREATE OR REPLACE TABLE `{PROJECT}.{DATASET}.{fam}` AS "+ " UNION ALL ".join(sels)
              subprocess.check_call(
                  ["bq","--project_id",PROJECT,"query","--use_legacy_sql=false",sql]
              )
              print(f"OK UNION {fam}")
          PY
