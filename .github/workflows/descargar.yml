name: SEACE CSV ZIP -> BigQuery (staging mensual + UNION final)

on:
  workflow_dispatch: {}

# Evita que 2 ejecuciones del mismo workflow se pisen.
concurrency:
  group: seace-bq-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  seace_union_bq:
    runs-on: ubuntu-latest

    # Un solo lugar para configurar todo
    env:
      PROJECT_ID: heroic-ruler-481618-e5
      FINAL_DS: github_actions
      STG_DS: github_actions_stg
      # Meses en formato YYYY-MM separados por coma. Puedes poner 2, 3, o más.
      MONTHS: "2026-01,2025-12,2025-11"

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}

      - name: Limpieza local (tmp/ data/)
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p tmp/zips tmp/unzipped data
          rm -rf tmp/unzipped/* data/* tmp/zips/* || true
          echo "OK: Limpieza local lista"

      - name: Preparar datasets y limpiar tablas (solo al inicio)
        shell: bash
        run: |
          set -euxo pipefail

          # 1) Asegura datasets (déy-ta-sets) existan
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$FINAL_DS" || true
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$STG_DS"   || true

          # 2) Borra todas las tablas del dataset final
          FINAL_TABLES=$(bq --project_id="$PROJECT_ID" ls --format=csv --max_results=100000 "$PROJECT_ID:$FINAL_DS" \
            | tail -n +2 | cut -d, -f1 || true)
          if [ -n "${FINAL_TABLES}" ]; then
            echo "Borrando tablas en $FINAL_DS"
            while IFS= read -r t; do
              echo "  - $FINAL_DS.$t"
              bq --project_id="$PROJECT_ID" rm -f -t "$PROJECT_ID:$FINAL_DS.$t" || true
            done <<< "${FINAL_TABLES}"
          else
            echo "No hay tablas que borrar en $FINAL_DS"
          fi

          # 3) Borra todas las tablas del dataset staging
          STG_TABLES=$(bq --project_id="$PROJECT_ID" ls --format=csv --max_results=100000 "$PROJECT_ID:$STG_DS" \
            | tail -n +2 | cut -d, -f1 || true)
          if [ -n "${STG_TABLES}" ]; then
            echo "Borrando tablas en $STG_DS"
            while IFS= read -r t; do
              echo "  - $STG_DS.$t"
              bq --project_id="$PROJECT_ID" rm -f -t "$PROJECT_ID:$STG_DS.$t" || true
            done <<< "${STG_TABLES}"
          else
            echo "No hay tablas que borrar en $STG_DS"
          fi

          echo "OK: Limpieza BigQuery terminada"

      - name: Descargar ZIPs SEACE (con retries, sin desincronizar meses)
        shell: bash
        run: |
          set -euxo pipefail

          # Convierte "YYYY-MM,YYYY-MM" a array
          IFS=',' read -r -a MONTH_LIST <<< "$MONTHS"

          # Validación mínima
          if [ "${#MONTH_LIST[@]}" -eq 0 ]; then
            echo "MONTHS está vacío. Define al menos 1 mes."
            exit 1
          fi

          for ym in "${MONTH_LIST[@]}"; do
            ym="$(echo "$ym" | xargs)"  # trim
            if ! [[ "$ym" =~ ^[0-9]{4}-[0-9]{2}$ ]]; then
              echo "Formato inválido en MONTHS: '$ym' (esperado YYYY-MM)"
              exit 1
            fi

            YEAR="${ym%-*}"
            MONTH="${ym#*-}"

            URL="https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/${YEAR}/${MONTH}"
            ZIP="tmp/zips/seace_${YEAR}_${MONTH}.zip"

            echo "Descargando $ym -> $ZIP"
            curl -L -f -sS \
              --retry 6 --retry-all-errors --retry-delay 2 \
              "$URL" -o "$ZIP"

            test -s "$ZIP"

            # Chequeo rápido: debe ser zip (sip)
            file "$ZIP" | grep -Ei "zip|archive" >/dev/null
          done

          echo "ZIPs descargados:"
          ls -lah tmp/zips

      - name: Cargar staging por mes + crear tablas finales con UNION en BigQuery
        shell: bash
        run: |
          set -euxo pipefail

          python - <<'PY'
          import os, re, glob, shutil, subprocess, sys
          from collections import defaultdict

          PROJECT_ID = os.environ["PROJECT_ID"]
          FINAL_DS   = os.environ["FINAL_DS"]
          STG_DS     = os.environ["STG_DS"]

          ZIPS_DIR  = "tmp/zips"
          UNZIP_DIR = "tmp/unzipped"
          DATA_DIR  = "data"

          os.makedirs(UNZIP_DIR, exist_ok=True)
          os.makedirs(DATA_DIR, exist_ok=True)

          def sh(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              print(r.stdout)
              return r.returncode, r.stdout

          def normalize_headers_line(header: str):
              cols = header.rstrip("\n\r").split(",")
              fixed, seen = [], {}
              for c in cols:
                  c = c.strip()
                  c = c.replace("/", "_").replace(" ", "_").replace("-", "_")
                  c = re.sub(r"[^A-Za-z0-9_]", "_", c)
                  c = re.sub(r"_+", "_", c).strip("_")
                  if not c:
                      c = "col"
                  if c[0].isdigit():
                      c = "c_" + c

                  base = c
                  k = seen.get(base, 0)
                  if k > 0:
                      c = f"{base}_{k}"
                  seen[base] = k + 1
                  fixed.append(c)
              return ",".join(fixed) + "\n"

          def safe_table_name(name_no_ext: str) -> str:
              t = re.sub(r"[^A-Za-z0-9_]", "_", name_no_ext)
              t = re.sub(r"_+", "_", t).strip("_")
              if not t:
                  t = "tabla"
              if t[0].isdigit():
                  t = "t_" + t
              return t

          # Descubre ZIPs reales descargados (evita desincronización)
          zip_files = sorted(glob.glob(os.path.join(ZIPS_DIR, "seace_????_??.zip")))
          if not zip_files:
              print("[ERROR] No hay ZIPs en tmp/zips. Revisa el paso de descarga.")
              sys.exit(1)

          stg_tables_by_logical = defaultdict(list)

          # 1) Cargar staging por mes
          for zip_path in zip_files:
              m = re.search(r"seace_(\d{4})_(\d{2})\.zip$", os.path.basename(zip_path))
              if not m:
                  print(f"[WARN] ZIP con nombre inesperado, se ignora: {zip_path}")
                  continue

              year, month = m.group(1), m.group(2)
              ym = f"{year}-{month}"

              if os.path.getsize(zip_path) == 0:
                  print(f"[ERROR] ZIP vacío: {zip_path}")
                  sys.exit(1)

              print("\n==============================")
              print(f"== STAGING MES: {ym}")
              print("==============================")

              # Limpieza entre meses (garantiza “no mezcla” local)
              shutil.rmtree(UNZIP_DIR, ignore_errors=True)
              shutil.rmtree(DATA_DIR, ignore_errors=True)
              os.makedirs(UNZIP_DIR, exist_ok=True)
              os.makedirs(DATA_DIR, exist_ok=True)

              # Unzip
              rc, _ = sh(["unzip", "-o", zip_path, "-d", UNZIP_DIR])
              if rc != 0:
                  sys.exit(rc)

              csv_paths = sorted(glob.glob(os.path.join(UNZIP_DIR, "**", "*.csv"), recursive=True))
              if not csv_paths:
                  print(f"[WARN] No hay CSVs en {ym}. Se salta ese mes.")
                  continue

              for src in csv_paths:
                  base = os.path.basename(src)
                  logical = safe_table_name(os.path.splitext(base)[0])

                  # Tabla staging mensual: logical__YYYY_MM
                  stg_table = f"{logical}__{year}_{month}"
                  stg_full  = f"{STG_DS}.{stg_table}"
                  out_clean = os.path.join(DATA_DIR, f"{stg_table}.csv")

                  # Leer binario, quitar NUL y normalizar header
                  with open(src, "rb") as f:
                      b = f.read().replace(b"\x00", b"")
                  lines = b.decode("utf-8", errors="replace").splitlines(True)
                  if not lines:
                      continue

                  # Header normalizado + anio_mes al inicio
                  norm_header = normalize_headers_line(lines[0]).strip()
                  new_lines = [f"anio_mes,{norm_header}\n"]
                  for line in lines[1:]:
                      if line.strip() == "":
                          continue
                      new_lines.append(f"{ym}," + line)

                  with open(out_clean, "w", encoding="utf-8", newline="") as g:
                      g.writelines(new_lines)

                  # Cargar staging (replace siempre para esa tabla mensual)
                  cmd = [
                      "bq", f"--project_id={PROJECT_ID}", "load",
                      "--source_format=CSV",
                      "--autodetect",
                      "--skip_leading_rows=1",
                      "--allow_quoted_newlines",
                      "--allow_jagged_rows",
                      "--max_bad_records=5000",
                      "--replace",
                      stg_full,
                      out_clean
                  ]

                  print(f"\n=== STAGING load: {base} -> {PROJECT_ID}:{stg_full} ===")
                  rc, _ = sh(cmd)
                  if rc != 0:
                      print(f"[ERROR] Falló staging load: {src} -> {stg_full}")
                      sys.exit(rc)

                  stg_tables_by_logical[logical].append(stg_table)

          if not stg_tables_by_logical:
              print("[ERROR] No se cargó ninguna tabla staging. Nada que consolidar.")
              sys.exit(1)

          # 2) Consolidar en BigQuery: FINAL = UNION ALL de staging
          #    Reglas:
          #    - Nunca ignorar una tabla lógica si existe en algún mes.
          #    - Descartar columnas que no coinciden (usamos SOLO columnas comunes).
          #    - Castear a STRING para evitar choques de tipos.
          for logical, stg_list in stg_tables_by_logical.items():
              stg_list = [t for t in stg_list if t]
              if not stg_list:
                  continue

              print("\n==============================")
              print(f"== FINAL TABLE: {PROJECT_ID}.{FINAL_DS}.{logical}")
              print("==============================")

              # Columnas comunes (intersección) en todas las staging del mismo logical
              table_names_sql = ", ".join([f"'{t}'" for t in stg_list])
              q_cols = f"""
              SELECT column_name
              FROM `{PROJECT_ID}.{STG_DS}.INFORMATION_SCHEMA.COLUMNS`
              WHERE table_name IN ({table_names_sql})
                AND column_name != 'anio_mes'
              GROUP BY column_name
              HAVING COUNT(DISTINCT table_name) = {len(stg_list)}
              ORDER BY column_name
              """

              rc, out = sh([
                  "bq", f"--project_id={PROJECT_ID}", "query",
                  "--nouse_legacy_sql",
                  "--format=csv",
                  q_cols
              ])
              if rc != 0:
                  print(f"[ERROR] No se pudo obtener columnas comunes para {logical}")
                  sys.exit(rc)

              # Parse CSV: primera línea es header
              rows = [r.strip() for r in out.splitlines() if r.strip()]
              common_cols = rows[1:] if len(rows) >= 2 else []

              # SELECT expr: siempre incluye anio_mes
              select_exprs = ["anio_mes"]
              for c in common_cols:
                  # CAST a STRING para evitar que un mes sea NUMERIC y otro STRING, etc.
                  select_exprs.append(f"CAST({c} AS STRING) AS {c}")

              union_parts = []
              for t in stg_list:
                  union_parts.append(
                      "SELECT " + ", ".join(select_exprs) + f" FROM `{PROJECT_ID}.{STG_DS}.{t}`"
                  )

              final_sql = f"""
              CREATE OR REPLACE TABLE `{PROJECT_ID}.{FINAL_DS}.{logical}` AS
              {("\nUNION ALL\n").join(union_parts)}
              """

              rc, _ = sh([
                  "bq", f"--project_id={PROJECT_ID}", "query",
                  "--nouse_legacy_sql",
                  final_sql
              ])
              if rc != 0:
                  print(f"[ERROR] Falló creación final para {logical}")
                  sys.exit(rc)

          # 3) Limpieza: borrar staging al final (para que no quede basura)
          #    Esto no afecta las tablas finales.
          print("\n== Limpieza staging (drop tables) ==")
          for logical, stg_list in stg_tables_by_logical.items():
              for t in stg_list:
                  rc, _ = sh([
                      "bq", f"--project_id={PROJECT_ID}", "rm",
                      "-f", "-t", f"{PROJECT_ID}:{STG_DS}.{t}"
                  ])
                  if rc != 0:
                      print(f"[WARN] No se pudo borrar staging {STG_DS}.{t} (se continúa)")

          print("\nOK: staging cargado, tablas finales creadas con UNION ALL en BigQuery, staging limpiado.")
          PY
