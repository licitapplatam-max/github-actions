name: SEACE CSV ZIP -> BigQuery (todas las tablas, 1 mes)

on:
  workflow_dispatch: {}

jobs:
  seace_zip_to_bq:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # Paso 1: Limpieza REAL
      - name: Limpiar data/ y tmp/
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p data tmp
          rm -rf data/*
          rm -rf tmp/*
          echo "data/ limpio:"
          ls -lah data

      # Paso 2: Descargar ZIP (sí, la ruta csv/... devuelve ZIP)
      - name: Descargar ZIP SEACE (csv)
        shell: bash
        run: |
          set -euxo pipefail
          YEAR="2026"
          MONTH="01"

          URL="https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/${YEAR}/${MONTH}"
          ZIP="tmp/seace_${YEAR}_${MONTH}.zip"

          curl -L -f -sS "$URL" -o "$ZIP"
          test -s "$ZIP"

          echo "Tipo:"
          file "$ZIP"

          echo "Contenido ZIP (primeros 50):"
          unzip -l "$ZIP" | head -n 50

      # Paso 3: Extraer TODOS los CSV del ZIP a tmp/unzipped
      - name: Extraer CSVs
        shell: bash
        run: |
          set -euxo pipefail
          unzip -o "tmp/seace_2026_01.zip" -d "tmp/unzipped"

          echo "CSVs encontrados:"
          find tmp/unzipped -type f -iname "*.csv" -print

          COUNT=$(find tmp/unzipped -type f -iname "*.csv" | wc -l)
          echo "Total CSV: $COUNT"
          test "$COUNT" -gt 0

      # Paso 4: Normalizar headers + cargar cada CSV a BigQuery
      - name: Normalizar headers y cargar cada CSV a BigQuery
        shell: bash
        run: |
          set -euxo pipefail

          PROJECT_ID="heroic-ruler-481618-e5"
          DATASET="github_actions"

          # (Opcional) crear dataset si no existe (no falla si ya existe)
          bq --project_id="$PROJECT_ID" mk --dataset --if_not_exists "$PROJECT_ID:$DATASET"

          python - <<'PY'
          import os, re, glob, subprocess, sys

          PROJECT_ID = "heroic-ruler-481618-e5"
          DATASET     = "github_actions"

          def normalize_headers_line(header: str):
              cols = header.rstrip("\n\r").split(",")
              fixed = []
              seen = {}
              for c in cols:
                  c = c.strip()

                  c = c.replace("/", "_")
                  c = c.replace(" ", "_")
                  c = c.replace("-", "_")
                  c = re.sub(r"[^A-Za-z0-9_]", "_", c)
                  c = re.sub(r"_+", "_", c).strip("_")

                  if not c:
                      c = "col"
                  if c[0].isdigit():
                      c = "c_" + c

                  base = c
                  k = seen.get(base, 0)
                  if k > 0:
                      c = f"{base}_{k}"
                  seen[base] = k + 1

                  fixed.append(c)

              return ",".join(fixed) + "\n"

          csv_paths = sorted(glob.glob("tmp/unzipped/**/*.csv", recursive=True))
          if not csv_paths:
              print("No se encontraron CSVs para procesar.")
              sys.exit(1)

          for src in csv_paths:
              base = os.path.basename(src)
              name_no_ext = os.path.splitext(base)[0]

              # Nombre tabla BigQuery: solo letras/numeros/_ (y no empezar con numero)
              table = re.sub(r"[^A-Za-z0-9_]", "_", name_no_ext)
              table = re.sub(r"_+", "_", table).strip("_")
              if not table:
                  table = "tabla"
              if table[0].isdigit():
                  table = "t_" + table

              # rutas output
              out_clean = os.path.join("data", f"{table}.csv")

              # 1) leer binario, quitar NULs
              with open(src, "rb") as f:
                  b = f.read().replace(b"\x00", b"")

              # 2) decodificar lo mejor posible (si hay caracteres raros, reemplaza)
              text = b.decode("utf-8", errors="replace").splitlines(True)
              if not text:
                  print(f"[SKIP] {base}: archivo vacío")
                  continue

              # 3) normalizar header (primera línea)
              text[0] = normalize_headers_line(text[0])

              # 4) escribir CSV listo para BigQuery
              os.makedirs("data", exist_ok=True)
              with open(out_clean, "w", encoding="utf-8", newline="") as g:
                  g.writelines(text)

              # 5) cargar a BigQuery (reemplaza tabla cada vez)
              full_table = f"{DATASET}.{table}"

              cmd = [
                  "bq", f"--project_id={PROJECT_ID}", "load",
                  "--source_format=CSV",
                  "--autodetect",
                  "--skip_leading_rows=1",
                  "--allow_quoted_newlines",
                  "--allow_jagged_rows",
                  "--max_bad_records=5000",
                  "--replace",
                  full_table,
                  out_clean
              ]

              print(f"\n=== Cargando {base} -> {full_table} ===")
              print("CMD:", " ".join(cmd))
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              print(r.stdout)

              if r.returncode != 0:
                  print(f"[ERROR] Falló carga de {base} -> {full_table}")
                  sys.exit(r.returncode)

          print("\nOK: Todas las tablas cargadas.")
          PY

      # Paso 5: Mostrar qué quedó en data/ (solo para que veas)
      - name: Ver data/
        shell: bash
        run: |
          set -euxo pipefail
          ls -lah data
