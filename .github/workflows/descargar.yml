name: SEACE CSV ZIP -> BigQuery (3 meses + contadores reales)

on:
  workflow_dispatch: {}

jobs:
  seace_3months_counts:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      # Define tus 3 meses aquí (AÑO-MES)
      MONTHS: "2025-11 2025-12 2026-01"

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # Paso 0: Dataset existe (no borra nada)
      - name: Crear dataset si no existe
        shell: bash
        run: |
          set -euxo pipefail
          bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" || true

      # Paso 1: Limpieza runner
      - name: Limpiar data/ y tmp/ (runner)
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p data tmp
          rm -rf data/* || true
          rm -rf tmp/*  || true
          echo "Runner limpio."

      # Paso 2: Descargar, extraer y contar filas (3 meses)
      - name: Descargar ZIPs + extraer CSVs + contar filas por CSV y total
        shell: bash
        run: |
          set -euxo pipefail

          python - <<'PY'
          import os, sys, re, csv, glob, zipfile, subprocess
          from pathlib import Path
          from collections import defaultdict

          PROJECT_ID = os.environ["PROJECT_ID"]
          DATASET    = os.environ["DATASET"]
          MONTHS     = os.environ["MONTHS"].split()

          tmp = Path("tmp")
          data = Path("data")
          tmp.mkdir(exist_ok=True)
          data.mkdir(exist_ok=True)

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              if r.returncode != 0:
                  print(r.stdout)
                  raise SystemExit(r.returncode)
              return r.stdout

          # Contadores:
          # per_month_file[(ym, filename)] = (total_lines, data_rows)
          per_month_file = {}
          # per_month_family[(ym, family)] = data_rows_sum
          per_month_family = defaultdict(int)
          # family_total[family] = data_rows_sum over all months
          family_total = defaultdict(int)

          def count_lines_fast(path: Path) -> int:
              # cuenta líneas rápido (binary). Si hay líneas raras, igual cuenta saltos de línea.
              with path.open("rb") as f:
                  return sum(1 for _ in f)

          for ym in MONTHS:
              year, month = ym.split("-")
              url = f"https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/{year}/{month}"
              zip_path = tmp / f"seace_{year}_{month}.zip"
              out_dir  = tmp / f"unzipped_{year}_{month}"

              print(f"\n=== MES {ym} ===")
              print(f"URL: {url}")

              # Descargar
              run(["bash","-lc", f"curl -L -f -sS '{url}' -o '{zip_path}'"])
              if zip_path.stat().st_size == 0:
                  raise SystemExit(f"ZIP vacío: {zip_path}")

              # Extraer
              out_dir.mkdir(parents=True, exist_ok=True)
              with zipfile.ZipFile(zip_path, "r") as z:
                  z.extractall(out_dir)

              csv_files = sorted(out_dir.glob("*.csv"))
              print(f"CSVs encontrados: {len(csv_files)}")
              if not csv_files:
                  raise SystemExit(f"No hay CSVs en {zip_path}")

              # Contar filas por CSV
              for f in csv_files:
                  total_lines = count_lines_fast(f)
                  data_rows = max(total_lines - 1, 0)  # asume 1 header
                  per_month_file[(ym, f.name)] = (total_lines, data_rows)

                  family = f.stem  # records, releases, com_parties, etc.
                  per_month_family[(ym, family)] += data_rows
                  family_total[family] += data_rows

              # Imprimir detalle por CSV (este mes)
              print("\n--- Filas por CSV (este mes) ---")
              for f in csv_files:
                  tl, dr = per_month_file[(ym, f.name)]
                  print(f"{f.name:35s} total_lines={tl:8d} data_rows={dr:8d}")

              # Resumen por familia (este mes)
              print("\n--- Total por tabla/familia (este mes) ---")
              fams = sorted([k for (m,k) in per_month_family.keys() if m == ym])
              month_sum = 0
              for fam in fams:
                  c = per_month_family[(ym, fam)]
                  month_sum += c
                  print(f"{fam:35s} data_rows_sum={c:10d}")
              print(f"\nTOTAL filas (data_rows) mes {ym}: {month_sum}")

          # Resumen total 3 meses
          print("\n==============================")
          print("RESUMEN TOTAL (3 meses)")
          print("==============================")
          grand_total = 0
          for fam in sorted(family_total.keys()):
              c = family_total[fam]
              grand_total += c
              print(f"{fam:35s} TOTAL_3MESES={c:12d}")
          print(f"\nGRAN TOTAL (data_rows) 3 meses: {grand_total}")

          # Guardar resumen a archivo para que quede en logs y puedas bajarlo como artifact si quieres después
          summary_path = tmp / "resumen_conteos.txt"
          with summary_path.open("w", encoding="utf-8") as w:
              w.write("DETALLE POR CSV (mes, archivo, total_lines, data_rows)\n")
              for (ym, fn), (tl, dr) in sorted(per_month_file.items()):
                  w.write(f"{ym}\t{fn}\t{tl}\t{dr}\n")
              w.write("\nTOTAL POR FAMILIA (mes, familia, data_rows_sum)\n")
              for (ym, fam), c in sorted(per_month_family.items()):
                  w.write(f"{ym}\t{fam}\t{c}\n")
              w.write("\nTOTAL 3 MESES POR FAMILIA\n")
              for fam, c in sorted(family_total.items()):
                  w.write(f"{fam}\t{c}\n")

          print(f"\nResumen guardado en: {summary_path}")
          PY

      - name: Mostrar resumen (archivo)
        shell: bash
        run: |
          set -euxo pipefail
          echo "==== tmp/resumen_conteos.txt ===="
          sed -n '1,200p' tmp/resumen_conteos.txt
          echo "==== (si es largo, solo mostré 200 líneas) ===="
