name: SEACE CSV ZIP -> BigQuery (3 meses FULL CLEAN + carga + union estable)

on:
  workflow_dispatch: {}

jobs:
  seace_3months_full:
    runs-on: ubuntu-latest

    env:
      PROJECT_ID: heroic-ruler-481618-e5
      DATASET: github_actions
      MONTHS: "2025-11 2025-12 2026-01"
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth Google
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # 0) Crear dataset si no existe (cachea el "already exists" sin romper nada)
      - name: Crear dataset si no existe (cache)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p tmp
          : > tmp/errors.txt

          # No cambia el flujo: sigue siendo mk || true, solo que ahora cacheamos el texto si aparece.
          out="$(bq --project_id="$PROJECT_ID" mk --dataset "$PROJECT_ID:$DATASET" 2>&1 || true)"
          if echo "$out" | grep -qi "already exists"; then
            echo -e "DATASET_ALREADY_EXISTS\t${PROJECT_ID}:${DATASET}" >> tmp/errors.txt
          fi

      # 1) LIMPIEZA TOTAL BigQuery (borra TODO lo que encuentre: tablas y vistas)
      - name: LIMPIEZA TOTAL BigQuery (tablas + vistas)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, subprocess

          PROJECT = os.environ["PROJECT_ID"]
          DATASET = os.environ["DATASET"]

          out = subprocess.check_output(["bq", "--project_id", PROJECT, "ls", DATASET], text=True)
          lines = out.splitlines()

          names = []
          for ln in lines[2:]:
              parts = ln.split()
              if parts:
                  names.append(parts[0])

          if not names:
              print("Dataset ya estaba vacío.")
          else:
              for name in names:
                  # Esto sí es útil: saber qué borró (pero no imprimimos cada cosa con miles de líneas extra).
                  print(f"Borrando: {DATASET}.{name}")
                  subprocess.check_call(["bq", "--project_id", PROJECT, "rm", "-f", "-t", f"{DATASET}.{name}"])

          print("OK: limpieza BigQuery completa.")
          PY

      # 2) Limpieza runner (sin spam) + asegurar cache
      - name: Limpiar runner (data/ tmp/)
        shell: bash
        run: |
          set -euo pipefail
          rm -rf data tmp || true
          mkdir -p data tmp
          : > tmp/errors.txt
          echo "Runner limpio. Cache: tmp/errors.txt"

      # 3) Descargar ZIPs + extraer + expected.tsv (resumen corto, sin imprimir cada archivo)
      - name: Descargar ZIPs + extraer + expected.tsv (sin spam)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, zipfile, subprocess
          from pathlib import Path

          MONTHS = os.environ["MONTHS"].split()
          tmp = Path("tmp"); tmp.mkdir(exist_ok=True)
          err = tmp / "errors.txt"

          def run(cmd):
              r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
              if r.returncode != 0:
                  # Cache del error real
                  err.write_text(err.read_text(encoding="utf-8") + f"CMD_FAIL\t{' '.join(cmd)}\n", encoding="utf-8")
                  print(r.stdout)
                  raise SystemExit(r.returncode)

          def count_lines_fast(path: Path) -> int:
              with path.open("rb") as f:
                  return sum(1 for _ in f)

          expected_lines = ["family\tym\texpected_rows"]

          for ym in MONTHS:
              y, m = ym.split("-")
              url = f"https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/{y}/{m}"
              zip_path = tmp / f"seace_{y}_{m}.zip"
              out_dir  = tmp / f"unzipped_{y}_{m}"

              print(f"=== MES {ym} ===")

              run(["bash","-lc", f"curl -L -f -sS '{url}' -o '{zip_path}'"])
              if zip_path.stat().st_size == 0:
                  with err.open("a", encoding="utf-8") as f:
                      f.write(f"ZIP_EMPTY\t{ym}\t{zip_path}\n")
                  raise SystemExit(f"ZIP vacío: {zip_path}")

              out_dir.mkdir(parents=True, exist_ok=True)
              with zipfile.ZipFile(zip_path, "r") as z:
                  z.extractall(out_dir)

              csvs = sorted(out_dir.glob("*.csv"))
              if not csvs:
                  with err.open("a", encoding="utf-8") as f:
                      f.write(f"NO_CSVS\t{ym}\t{zip_path}\n")
                  raise SystemExit(f"No hay CSVs en {zip_path}")

              # resumimos por familia SIN imprimir cada CSV
              fam_rows = {}
              for f in csvs:
                  total_lines = count_lines_fast(f)
                  data_rows = max(total_lines - 1, 0)
                  fam_rows[f.stem] = fam_rows.get(f.stem, 0) + data_rows

              for fam, rows in sorted(fam_rows.items()):
                  expected_lines.append(f"{fam}\t{ym}\t{rows}")

              print(f"CSVs: {len(csvs)} | Familias: {len(fam_rows)}")

          (tmp/"expected.tsv").write_text("\n".join(expected_lines) + "\n", encoding="utf-8")
          print("OK: tmp/expected.tsv creado")
          PY

      # 4) Normalizar headers + schema TODO STRING + guardar CSV limpio en data/ (ok)
      - name: Normalizar headers + schema TODO STRING
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, json
          from pathlib import Path

          MONTHS = os.environ["MONTHS"].split()
          tmp  = Path("tmp")
          data = Path("data"); data.mkdir(exist_ok=True)

          def norm_name(s: str) -> str:
              s = s.strip().replace("/", "_").replace(" ", "_").replace("-", "_")
              s = re.sub(r"[^A-Za-z0-9_]", "_", s)
              s = re.sub(r"_+", "_", s).strip("_")
              if not s:
                  s = "col"
              if s[0].isdigit():
                  s = "c_" + s
              return s

          def normalize_header(line: str):
              cols = line.rstrip("\n\r").split(",")
              fixed = []
              seen = {}
              for c in cols:
                  c = norm_name(c)
                  k = seen.get(c, 0)
                  seen[c] = k + 1
                  if k > 0:
                      c = f"{c}_{k}"
                  fixed.append(c)
              return fixed

          total = 0
          for ym in MONTHS:
              y, m = ym.split("-")
              suffix = f"{y}_{m}"
              src_dir = tmp / f"unzipped_{y}_{m}"
              for src in sorted(src_dir.glob("*.csv")):
                  base = norm_name(src.stem)
                  table = f"{base}_{suffix}"

                  b = src.read_bytes().replace(b"\x00", b"")
                  lines = b.decode("utf-8", errors="replace").splitlines(True)
                  if not lines:
                      continue

                  header_cols = normalize_header(lines[0])
                  lines[0] = ",".join(header_cols) + "\n"

                  out_csv = data / f"{table}.csv"
                  out_csv.write_text("".join(lines), encoding="utf-8", newline="")

                  schema = [{"name": c, "type": "STRING", "mode": "NULLABLE"} for c in header_cols]
                  (data / f"{table}.schema.json").write_text(json.dumps(schema), encoding="utf-8")

                  total += 1

          print(f"OK: CSVs limpios creados: {total}")
          PY

      # 5) Cargar CSVs a BigQuery (cachea fallas por tabla, sin spam)
      - name: Cargar CSVs a BigQuery (cache de fallas)
        shell: bash
        run: |
          set -euo pipefail
          # Si alguna carga falla, la registramos y seguimos para ver el panorama completo.
          set +e
          for csv in data/*.csv; do
            table=$(basename "$csv" .csv)
            bq --project_id="$PROJECT_ID" load \
              --replace \
              --source_format=CSV \
              --skip_leading_rows=1 \
              --allow_quoted_newlines \
              --max_bad_records=0 \
              "$DATASET.$table" \
              "$csv" \
              "data/$table.schema.json"
            rc=$?
            if [ $rc -ne 0 ]; then
              echo -e "LOAD_FAIL\t$table\t$rc" >> tmp/errors.txt
            fi
          done
          set -e

      # 6) Unión por familia (cachea union fail por familia, sin romper)
      - name: Unir tablas por familia (cache de fallas)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, subprocess
          from collections import defaultdict
          from pathlib import Path

          PROJECT = os.environ["PROJECT_ID"]
          DATASET = os.environ["DATASET"]
          errfile = Path("tmp/errors.txt")

          out = subprocess.check_output(["bq","--project_id",PROJECT,"ls",DATASET], text=True)
          tables = [ln.split()[0] for ln in out.splitlines()[2:] if ln.strip()]
          monthly = [t for t in tables if re.search(r'_\d{4}_\d{2}$', t)]

          if not monthly:
              with errfile.open("a", encoding="utf-8") as f:
                  f.write("NO_MONTHLY_TABLES\tNo hay tablas *_YYYY_MM para unir\n")
              raise SystemExit("No hay tablas mensuales *_YYYY_MM para unir.")

          fams = defaultdict(list)
          for t in monthly:
              fam = re.sub(r'_\d{4}_\d{2}$', '', t)
              fams[fam].append(t)

          def get_cols(table):
              q = f"""
              SELECT column_name
              FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.COLUMNS`
              WHERE table_name = '{table}'
              ORDER BY ordinal_position
              """
              out = subprocess.check_output(
                  ["bq","--project_id",PROJECT,"query","--use_legacy_sql=false","--format=csv",q],
                  text=True
              )
              return [ln.strip() for ln in out.splitlines()[1:] if ln.strip()]

          # Ejecutamos uniones, pero si alguna falla la cacheamos y seguimos.
          for fam, ts in sorted(fams.items()):
              ts = sorted(ts)

              all_cols = []
              seen = set()
              cols_by_table = {}
              for t in ts:
                  cols = get_cols(t)
                  cols_by_table[t] = set(cols)
                  for c in cols:
                      if c not in seen:
                          seen.add(c)
                          all_cols.append(c)

              selects = []
              for t in ts:
                  present = cols_by_table[t]
                  exprs = []
                  for c in all_cols:
                      if c in present:
                          exprs.append(f"`{c}` AS `{c}`")
                      else:
                          exprs.append(f"CAST(NULL AS STRING) AS `{c}`")
                  selects.append(f"SELECT {', '.join(exprs)} FROM `{PROJECT}.{DATASET}.{t}`")

              union_sql = " UNION ALL ".join(selects)
              final_sql = f"CREATE OR REPLACE TABLE `{PROJECT}.{DATASET}.{fam}` AS {union_sql}"

              try:
                  subprocess.check_call(["bq","--project_id",PROJECT,"query","--use_legacy_sql=false",final_sql])
              except subprocess.CalledProcessError as e:
                  with errfile.open("a", encoding="utf-8") as f:
                      f.write(f"UNION_FAIL\t{fam}\t{e.returncode}\n")
                  # seguir con el resto
                  continue

          print("OK: unión por familia ejecutada (fallas, si hubo, están en tmp/errors.txt).")
          PY

      # 7) Diagnóstico (lo dejamos, pero ya no dependes solo de esto)
      - name: Diagnóstico (mensuales vs finales)
        shell: bash
        run: |
          set -euo pipefail
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false --format=pretty "
          WITH monthly AS (
            SELECT DISTINCT REGEXP_REPLACE(table_name, r'_[0-9]{4}_[0-9]{2}$', '') AS fam
            FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
            WHERE table_type='BASE TABLE'
              AND REGEXP_CONTAINS(table_name, r'_[0-9]{4}_[0-9]{2}$')
          ),
          finals AS (
            SELECT table_name AS fam
            FROM \`${PROJECT_ID}.${DATASET}.INFORMATION_SCHEMA.TABLES\`
            WHERE table_type='BASE TABLE'
              AND NOT REGEXP_CONTAINS(table_name, r'_[0-9]{4}_[0-9]{2}$')
          )
          SELECT m.fam, IF(f.fam IS NULL, 'FALTA_FINAL', 'OK') AS estado
          FROM monthly m
          LEFT JOIN finals f USING(fam)
          ORDER BY estado DESC, fam;
          "

      # 8) Verificación rápida records (cachea si falla)
      - name: Verificar records final (conteo) + cache fail
        shell: bash
        run: |
          set -euo pipefail
          set +e
          bq --project_id="$PROJECT_ID" query --use_legacy_sql=false --format=pretty "
          SELECT COUNT(1) AS filas_records_final
          FROM \`${PROJECT_ID}.${DATASET}.records\`
          "
          rc=$?
          if [ $rc -ne 0 ]; then
            echo -e "RECORDS_QUERY_FAIL\trecords\t$rc" >> tmp/errors.txt
          fi
          set -e

      # 9) Cache final: faltantes mensuales + faltantes finales + extras + mismatch de nombres (SIN SPAM)
      - name: Cache final (faltantes mensuales/finales + extras + mismatch)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, subprocess
          from pathlib import Path
          from collections import defaultdict

          PROJECT = os.environ["PROJECT_ID"]
          DATASET  = os.environ["DATASET"]
          MONTHS   = os.environ["MONTHS"].split()

          tmp = Path("tmp")
          err = tmp / "errors.txt"
          tsv = tmp / "expected.tsv"

          def norm_name(s: str) -> str:
              s = s.strip().replace("/", "_").replace(" ", "_").replace("-", "_")
              s = re.sub(r"[^A-Za-z0-9_]", "_", s)
              s = re.sub(r"_+", "_", s).strip("_")
              if not s:
                  s = "col"
              if s[0].isdigit():
                  s = "c_" + s
              return s

          if not tsv.exists():
              with err.open("a", encoding="utf-8") as f:
                  f.write("EXPECTED_MISSING\ttmp/expected.tsv no existe\n")
              print("CACHE: expected.tsv faltante.")
              raise SystemExit(0)

          # 1) Leer expected.tsv (familias "crudas") y construir esperado NORMALIZADO (como realmente nombras tablas)
          exp = defaultdict(set)         # ym -> set(raw fam)
          raw_to_norm = {}               # raw -> norm
          for ln in tsv.read_text(encoding="utf-8").splitlines()[1:]:
              fam_raw, ym, _ = ln.split("\t")
              exp[ym].add(fam_raw)
              fam_norm = norm_name(fam_raw)
              raw_to_norm[fam_raw] = fam_norm

          # warning si hay mismatch raw->norm (es el origen típico de "faltan mensuales/finales" falsos)
          mismatches = sorted([r for r,n in raw_to_norm.items() if r != n])

          expected_monthly = set()
          expected_final   = set()
          for ym in MONTHS:
              y, m = ym.split("-")
              suffix = f"{y}_{m}"
              for fam_raw in exp.get(ym, set()):
                  fam = raw_to_norm[fam_raw]
                  expected_monthly.add(f"{fam}_{suffix}")
                  expected_final.add(fam)

          # 2) Tablas reales
          out = subprocess.check_output(["bq","--project_id",PROJECT,"ls",DATASET], text=True)
          real = set()
          for ln in out.splitlines()[2:]:
              parts = ln.split()
              if parts:
                  real.add(parts[0])

          real_monthly = {t for t in real if re.search(r'_\d{4}_\d{2}$', t)}
          real_final   = {t for t in real if not re.search(r'_\d{4}_\d{2}$', t)}

          missing_monthly = sorted(expected_monthly - real_monthly)
          missing_final   = sorted(expected_final - real_final)

          extra_monthly = sorted(real_monthly - expected_monthly)
          extra_final   = sorted(real_final - expected_final)

          with err.open("a", encoding="utf-8") as f:
              for t in missing_monthly:
                  f.write(f"MISSING_MONTHLY\t{t}\n")
              for t in missing_final:
                  f.write(f"MISSING_FINAL\t{t}\n")
              for t in extra_monthly[:200]:
                  f.write(f"EXTRA_MONTHLY\t{t}\n")
              for t in extra_final[:200]:
                  f.write(f"EXTRA_FINAL\t{t}\n")
              for r in mismatches[:200]:
                  f.write(f"WARN_NAME_MISMATCH\t{r}\t{raw_to_norm[r]}\n")

          # Resumen corto (sin spam)
          print("=== CACHE RESUMEN ===")
          print(f"Missing monthly: {len(missing_monthly)} | Missing final: {len(missing_final)}")
          print(f"Extra monthly:   {len(extra_monthly)}   | Extra final:   {len(extra_final)}")
          print(f"Name mismatches: {len(mismatches)}")
          PY

      # 10) Mostrar cache útil (solo si hay algo)
      - name: Mostrar tmp/errors.txt (solo si hay entradas)
        shell: bash
        run: |
          set -euo pipefail
          if [ -s tmp/errors.txt ]; then
            echo "=== tmp/errors.txt ==="
            cat tmp/errors.txt
          else
            echo "OK: tmp/errors.txt vacío (no se detectaron fallas cacheables)."
          fi
