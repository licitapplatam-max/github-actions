name: Descargar CSV/ZIP SEACE y cargar a BigQuery (1 mes)

on:
  workflow_dispatch: {}

jobs:
  fetch_csv_bq:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout (chék-aut)
        uses: actions/checkout@v4

      - name: Auth Google (gúgol)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (yi-kláud)
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: heroic-ruler-481618-e5

      # 1) LIMPIEZA REAL: borra TODO dentro de data/ (incluye subcarpetas)
      - name: Limpiar carpeta data (real)
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p data tmp
          echo "ANTES (data/):"
          ls -lah data || true

          # borra todo lo que haya dentro (archivos y carpetas)
          rm -rf data/*

          echo "DESPUÉS (data/):"
          ls -lah data

          # limpia temporales también
          rm -rf tmp/*
          ls -lah tmp

      # 2) DESCARGA: puede venir como ZIP o como CSV
      - name: Descargar archivo (puede ser ZIP o CSV)
        shell: bash
        run: |
          set -euxo pipefail

          YEAR="2026"
          MONTH="01"

          URL="https://contratacionesabiertas.oece.gob.pe/api/v1/file/seace_v3/csv/${YEAR}/${MONTH}"
          RAW="tmp/seace_${YEAR}_${MONTH}.bin"

          echo "Descargando desde: $URL"
          curl -L -f -sS "$URL" -o "$RAW"
          test -s "$RAW"

          echo "Tipo detectado (file):"
          file "$RAW"

          # Si viene comprimido, extraemos CSV(s)
          if file "$RAW" | grep -qi "zip"; then
            echo "Vino como ZIP. Extrayendo..."
            unzip -o "$RAW" -d tmp/unzipped

            echo "Archivos dentro del ZIP:"
            find tmp/unzipped -type f -maxdepth 4 -print

            # agarramos el primer .csv que encontremos
            CSV_IN="$(find tmp/unzipped -type f -iname '*.csv' | head -n 1)"
            if [ -z "$CSV_IN" ]; then
              echo "ERROR: No se encontró ningun .csv dentro del ZIP"
              exit 1
            fi

            cp "$CSV_IN" "data/seace_v3_${YEAR}_${MONTH}.csv"
          else
            echo "Vino como texto. Guardando como CSV directo."
            cp "$RAW" "data/seace_v3_${YEAR}_${MONTH}.csv"
          fi

          echo "Archivo final en data/:"
          ls -lh data/

          echo "Primeras 5 líneas:"
          head -n 5 "data/seace_v3_${YEAR}_${MONTH}.csv"

      # 3) LIMPIEZA ASCII 0 (si aparece)
      - name: Quitar ASCII 0 (NUL) si existe
        shell: bash
        run: |
          set -euxo pipefail

          YEAR="2026"
          MONTH="01"
          IN="data/seace_v3_${YEAR}_${MONTH}.csv"
          OUT="data/seace_v3_${YEAR}_${MONTH}_clean.csv"

          python - <<'PY'
          p="data/seace_v3_2026_01.csv"
          b=open(p,"rb").read()
          print("Bytes:", len(b))
          print("NULs:", b.count(b"\x00"))
          PY

          NULS=$(python - <<'PY'
          b=open("data/seace_v3_2026_01.csv","rb").read()
          print(b.count(b"\x00"))
          PY
          )

          if [ "$NULS" != "0" ]; then
            echo "Se detectaron NULs ($NULS). Limpiando..."
            tr -d '\000' < "$IN" > "$OUT"
          else
            echo "No hay NULs. Copiando sin cambios..."
            cp "$IN" "$OUT"
          fi

          test -s "$OUT"
          echo "Preview clean:"
          head -n 3 "$OUT"

      # 4) CARGA A BIGQUERY (big-kuéri)
      - name: Cargar a BigQuery (big-kuéri)
        shell: bash
        run: |
          set -euxo pipefail

          PROJECT_ID="heroic-ruler-481618-e5"
          DATASET="github_actions"
          TABLE="seace_v3_2026_01"
          FILE="data/seace_v3_2026_01_clean.csv"

          echo "Cargando a ${PROJECT_ID}:${DATASET}.${TABLE}"
          ls -lh "$FILE"

          bq --project_id="$PROJECT_ID" load \
            --source_format=CSV \
            --autodetect \
            --skip_leading_rows=1 \
            --allow_quoted_newlines \
            --allow_jagged_rows \
            --max_bad_records=5000 \
            --replace \
            "${DATASET}.${TABLE}" \
            "$FILE"

          echo "OK. Tabla lista."
